{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial ACS_UPB_LAB1: Running Sumo Simulations\n",
    "\n",
    "__Credits: most of the credits for this ipynb goes to https://github.com/flow-project/flow/tree/master/tutorials__\n",
    "\n",
    "This tutorial walks through the process of running non-RL traffic simulations in Flow. Simulations of this form act as non-autonomous baselines and depict the behavior of human dynamics on a network. Similar simulations may also be used to evaluate the performance of hand-designed controllers on a network. This tutorial focuses primarily on the former use case, while an example of the latter may be found in `exercise07_controllers.ipynb`.\n",
    "\n",
    "In this exercise, we simulate a initially perturbed single lane ring road. We witness in simulation that as time advances the initially perturbations do not dissipate, but instead propagates and expands until vehicles are forced to periodically stop and accelerate. For more information on this behavior, we refer the reader to the following article [1].\n",
    "\n",
    "## 1.1 Components of a Simulation\n",
    "All simulations, both in the presence and absence of RL, require two components: a *network*, and an *environment*. Networks describe the features of the transportation network used in simulation. This includes the positions and properties of nodes and edges constituting the lanes and junctions, as well as properties of the vehicles, traffic lights, inflows, etc. in the network. Environments, on the other hand, initialize, reset, and advance simulations, and act the primary interface between the reinforcement learning algorithm and the network. Moreover, custom environments may be used to modify the dynamical features of an network.\n",
    "\n",
    "## 1.2 Setting up the environment of current lab (ENV1)\n",
    "Load configurations for lab 1.\n",
    "\n",
    "## 2. Setting up a Network\n",
    "Flow contains a plethora of pre-designed networks used to replicate highways, intersections, and merges in both closed and open settings. All these networks are located in flow/networks. In order to recreate a ring road network, we begin by importing the network `RingNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FigureEightNetwork\n"
     ]
    }
   ],
   "source": [
    "from flow.envs.nemodrive_lab import ENV2 as ENV\n",
    "\n",
    "# from flow.networks.figure_eight import FigureEightNetwork\n",
    "network_name = ENV[\"NETWORK\"]\n",
    "print(network_name.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network, as well as all other networks in Flow, is parametrized by the following arguments: \n",
    "* name\n",
    "* vehicles\n",
    "* net_params\n",
    "* initial_config\n",
    "* traffic_lights\n",
    "\n",
    "These parameters allow a single network to be recycled for a multitude of different network settings. For example, `RingNetwork` may be used to create ring roads of variable length with a variable number of lanes and vehicles.\n",
    "\n",
    "### 2.1 Name\n",
    "The `name` argument is a string variable depicting the name of the network. This has no effect on the type of network created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = network_name.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 VehicleParams\n",
    "The `VehicleParams` class stores state information on all vehicles in the network. This class is used to identify the dynamical behavior of a vehicle and whether it is controlled by a reinforcement learning agent. Morover, information pertaining to the observations and reward function can be collected from various get methods within this class.\n",
    "\n",
    "The initial configuration of this class describes the number of vehicles in the network at the start of every simulation, as well as the properties of these vehicles. We begin by creating an empty `VehicleParams` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = ENV[\"VEHICLES\"]()\n",
    "\n",
    "# code in get_vehicles \n",
    "# from flow.core.params import VehicleParams\n",
    "\n",
    "# vehicles = VehicleParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this object is created, vehicles may be introduced using the `add` method. This method specifies the types and quantities of vehicles at the start of a simulation rollout. For a description of the various arguements associated with the `add` method, we refer the reader to the following documentation ([VehicleParams.add](https://flow.readthedocs.io/en/latest/flow.core.html?highlight=vehicleparam#flow.core.params.VehicleParams)).\n",
    "\n",
    "When adding vehicles, their dynamical behaviors may be specified either by the simulator (default), or by user-generated models. For longitudinal (acceleration) dynamics, several prominent car-following models are implemented in Flow. For this example, the acceleration behavior of all vehicles will be defined by the Intelligent Driver Model (IDM) [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in get_vehicles \n",
    "# from flow.controllers.car_following_models import IDMController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another controller we define is for the vehicle's routing behavior. For closed network where the route for any vehicle is repeated, the `ContinuousRouter` controller is used to perpetually reroute all vehicles to the initial set route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in get_vehicles \n",
    "# from flow.controllers.routing_controllers import ContinuousRouter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add 22 vehicles of type \"human\" with the above acceleration and routing behavior into the `Vehicles` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (E.g. code in get_vehicles)\n",
    "# vehicles.add(\"human\",\n",
    "#              acceleration_controller=(IDMController, {}),\n",
    "#              routing_controller=(ContinuousRouter, {}),\n",
    "#              num_vehicles=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 NetParams\n",
    "\n",
    "`NetParams` are network-specific parameters used to define the shape and properties of a network. Unlike most other parameters, `NetParams` may vary drastically depending on the specific network configuration, and accordingly most of its parameters are stored in `additional_params`. In order to determine which `additional_params` variables may be needed for a specific network, we refer to the `ADDITIONAL_NET_PARAMS` variable located in the network file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'radius_ring': 60, 'lanes': 2, 'speed_limit': 30, 'resolution': 40}\n"
     ]
    }
   ],
   "source": [
    "# from flow.networks.ring import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "ADDITIONAL_NET_PARAMS = ENV[\"ADDITIONAL_NET_PARAMS\"]\n",
    "\n",
    "print(ADDITIONAL_NET_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the `ADDITIONAL_NET_PARAMS` dict from the ring road network, we see that the required parameters are:\n",
    "\n",
    "* **length**: length of the ring road\n",
    "* **lanes**: number of lanes\n",
    "* **speed**: speed limit for all edges\n",
    "* **resolution**: resolution of the curves on the ring. Setting this value to 1 converts the ring to a diamond.\n",
    "\n",
    "\n",
    "At times, other inputs may be needed from `NetParams` to recreate proper network features/behavior. These requirements can be founded in the network's documentation. For the ring road, no attributes are needed aside from the `additional_params` terms. Furthermore, for this exercise, we use the network's default parameters when creating the `NetParams` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<flow.core.params.NetParams object at 0x7f26d263ebe0>\n"
     ]
    }
   ],
   "source": [
    "from flow.core.params import NetParams\n",
    "\n",
    "net_params = NetParams(additional_params=ADDITIONAL_NET_PARAMS)\n",
    "print(net_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 InitialConfig\n",
    "\n",
    "`InitialConfig` specifies parameters that affect the positioning of vehicle in the network at the start of a simulation. These parameters can be used to limit the edges and number of lanes vehicles originally occupy, and provide a means of adding randomness to the starting positions of vehicles. In order to introduce a small initial disturbance to the system of vehicles in the network, we set the `perturbation` term in `InitialConfig` to 1m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spacing': 'random', 'perturbation': 50}\n"
     ]
    }
   ],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "initial_config_param = ENV[\"INITIAL_CONFIG_PARAMS\"]\n",
    "print(initial_config_param)\n",
    "\n",
    "initial_config = InitialConfig(**initial_config_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 TrafficLightParams\n",
    "\n",
    "`TrafficLightParams` are used to describe the positions and types of traffic lights in the network. These inputs are outside the scope of this tutorial, and instead are covered in `exercise06_traffic_lights.ipynb`. For our example, we create an empty `TrafficLightParams` object, thereby ensuring that none are placed on any nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import TrafficLightParams\n",
    "\n",
    "traffic_lights = TrafficLightParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up an Environment\n",
    "\n",
    "Several envionrments in Flow exist to train autonomous agents of different forms (e.g. autonomous vehicles, traffic lights) to perform a variety of different tasks. These environments are often network or task specific; however, some can be deployed on an ambiguous set of networks as well. One such environment, `AccelEnv`, may be used to train a variable number of vehicles in a fully observable network with a *static* number of vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'flow.envs.nemodrive_lab.env2_lab.LaneChangeAccelEnv2'>\n"
     ]
    }
   ],
   "source": [
    "# from flow.envs.nemodrive_lab.env1_lab import LaneChangeAccelEnv1\n",
    "env_name = ENV[\"ENVIRONMENT\"]\n",
    "print(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we will not be training any autonomous agents in this exercise, the use of an environment allows us to view the cumulative reward simulation rollouts receive in the absence of autonomy.\n",
    "\n",
    "Envrionments in Flow are parametrized by three components:\n",
    "* `EnvParams`\n",
    "* `SumoParams`\n",
    "* `Network`\n",
    "\n",
    "### 3.1 SumoParams\n",
    "`SumoParams` specifies simulation-specific variables. These variables include the length a simulation step (in seconds) and whether to render the GUI when running the experiment. For this example, we consider a simulation step length of 0.1s and activate the GUI.\n",
    "\n",
    "Another useful parameter is `emission_path`, which is used to specify the path where the emissions output will be generated. They contain a lot of information about the simulation, for instance the position and speed of each car at each time step. If you do not specify any emission path, the emission file will not be generated. More on this in Section 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams(sim_step=0.1, render=True, emission_path='data', restart_instance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 EnvParams\n",
    "\n",
    "`EnvParams` specify environment and experiment-specific parameters that either affect the training process or the dynamics of various components within the network. Much like `NetParams`, the attributes associated with this parameter are mostly environment specific, and can be found in the environment's `ADDITIONAL_ENV_PARAMS` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_accel': 3, 'max_decel': 3, 'lane_change_duration': 0, 'target_velocity': 10, 'sort_vehicles': False, 'forward_progress_gain': 0.1, 'collision_reward': -1, 'lane_change_reward': -0.1, 'frontal_collision_distance': 2.0, 'lateral_collision_distance': 3.0, 'action_space_box': False, 'pos_noise_std': [0.5, 2], 'pos_noise_steps_reset': 100, 'speed_noise_std': [0.2, 0.8], 'acc_noise_std': [0.2, 0.4]}\n"
     ]
    }
   ],
   "source": [
    "# from flow.envs.nemodrive_lab.env1_lab import ADDITIONAL_ENV1_PARAMS\n",
    "ADDITIONAL_ENV_PARAMS = ENV[\"ADDITIONAL_ENV_PARAMS\"]\n",
    "\n",
    "print(ADDITIONAL_ENV_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the `ADDITIONAL_ENV_PARAMS` variable, we see that it consists of only one entry, \"target_velocity\", which is used when computing the reward function associated with the environment. We use this default value when generating the `EnvParams` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "env_params = EnvParams(additional_params=ADDITIONAL_ENV_PARAMS, horizon=ENV[\"HORIZON\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting up and Running the Experiment\n",
    "Once the inputs to the network and environment classes are ready, we are ready to set up a `Experiment` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.experiment import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These objects may be used to simulate rollouts in the absence of reinforcement learning agents, as well as acquire behaviors and rewards that may be used as a baseline with which to compare the performance of the learning agent. In this case, we choose to run our experiment for one rollout consisting of 3000 steps (300 s).\n",
    "\n",
    "**Note**: When executing the below code, remeber to click on the    <img style=\"display:inline;\" src=\"img/play_button.png\"> Play button after the GUI is rendered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network object\n",
    "network = network_name(name=\"ring_example\",\n",
    "                       vehicles=vehicles,\n",
    "                       net_params=net_params,\n",
    "                       initial_config=initial_config,\n",
    "                       traffic_lights=traffic_lights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during start: Traceback (most recent call last):\n",
      "  File \"/home/victor/Documents/AAIT/lab1/flow/flow/core/kernel/simulation/traci.py\", line 158, in start_simulation\n",
      "    traci_connection.setOrder(0)\n",
      "  File \"/home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/traci/connection.py\", line 348, in setOrder\n",
      "    self._sendExact()\n",
      "  File \"/home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/traci/connection.py\", line 99, in _sendExact\n",
      "    raise FatalTraCIError(\"connection closed by SUMO\")\n",
      "traci.exceptions.FatalTraCIError: connection closed by SUMO\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4070cc562e07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create the environment object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msumo_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msumo_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# create the experiment object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/AAIT/lab1/flow/flow/envs/nemodrive_lab/env2_lab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_params, sim_params, network, simulator)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc_noise_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc_noise_std\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crt_pos_noise_accum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/AAIT/lab1/flow/flow/envs/nemodrive_lab/env1_lab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_params, sim_params, network, simulator)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"action_space_box\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         self.num_lanes = max(self.k.network.num_lanes(edge)\n",
      "\u001b[0;32m~/Documents/AAIT/lab1/flow/flow/envs/ring/accel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_params, sim_params, network, simulator)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/AAIT/lab1/flow/flow/envs/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_params, sim_params, network, simulator, scenario)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# needs to be simulated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         kernel_api = self.k.simulation.start_simulation(\n\u001b[0;32m--> 167\u001b[0;31m             network=self.k.network, sim_params=sim_params)\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# pass the kernel api to the kernel and it's subclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/AAIT/lab1/flow/flow/core/kernel/simulation/traci.py\u001b[0m in \u001b[0;36mstart_simulation\u001b[0;34m(self, network, sim_params)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mtraci_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraci\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumRetries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mtraci_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetOrder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0mtraci_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulationStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/site-packages/traci/connection.py\u001b[0m in \u001b[0;36msetOrder\u001b[0;34m(self, order)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMD_SETORDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_string\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!BBi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMD_SETORDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sendExact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/site-packages/traci/connection.py\u001b[0m in \u001b[0;36m_sendExact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recvExact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/site-packages/traci/connection.py\u001b[0m in \u001b[0;36m_recvExact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create the environment object\n",
    "sumo_params.render = True\n",
    "env = env_name(env_params, sumo_params, network)\n",
    "\n",
    "# create the experiment object\n",
    "exp = Experiment(env)\n",
    "_ = exp.run(1, 3000, convert_to_csv=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run still agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumo_params.render = True\n",
    "env = env_name(env_params, sumo_params, network)\n",
    "\n",
    "# create the experiment object\n",
    "exp = Experiment(env)\n",
    "\n",
    "rl_actions = lambda state: [0, 0]\n",
    "\n",
    "_ = exp.run(1, 3000, convert_to_csv=True, rl_actions=rl_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run random agent.\n",
    "\n",
    "Use __FullExperiment__ to test agent that expects _state, reward, done, info_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0, return: 2045.934041388867\n",
      "Round 1, return: 3040.3029170426053\n",
      "Round 2, return: 3942.550049239418\n",
      "Round 3, return: 3474.2417063040443\n",
      "Round 4, return: 4647.296146049075\n",
      "Round 5, return: 3996.9543100739525\n",
      "Round 6, return: 4382.0542963642265\n",
      "Round 7, return: 2022.706060773645\n",
      "Round 8, return: 4056.83268915989\n",
      "Round 9, return: 4185.936077234212\n",
      "Average, std return: 3579.480829362993, 881.0908793593906\n",
      "Average, std speed: 6.257771848130913, 0.6050931947327768\n"
     ]
    }
   ],
   "source": [
    "from flow.core.experiment_with_reward import FullExperiment\n",
    "import numpy as np\n",
    "\n",
    "class RandomAgent():\n",
    "    def __init__(self, env):\n",
    "        self.action_space = env.action_space\n",
    "        self.max_decel = env.env_params.additional_params[\"max_decel\"]\n",
    "        self.max_accel = env.env_params.additional_params[\"max_accel\"]\n",
    "        self.max_speed = env.net_params.additional_params['speed_limit']\n",
    "        self.change_lane_step_freq = 1\n",
    "        self.num_steps = 0\n",
    "        self.prev_speed = None\n",
    "        \n",
    "    def split_state(self, state):\n",
    "        return np.split(state, 3)\n",
    "    \n",
    "    def change_lane(self, current_lane):\n",
    "        if current_lane > 0.25:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2\n",
    "        \n",
    "    def act(self, state, reward, done, info):\n",
    "        speed, pos, lane = self.split_state(state)\n",
    "        current_speed = speed[0] * self.max_speed\n",
    "        d = 1\n",
    "        if self.prev_speed is not None and self.prev_speed > current_speed:\n",
    "            d = self.change_lane(lane[0])\n",
    "        self.prev_speed = current_speed\n",
    "#         print(f'State speed {speed[0]:.5f}, pos {pos[0]:.2f}, lane {lane[0]:.2f}, reward {reward:.2f}')\n",
    "        self.num_steps += 1\n",
    "\n",
    "        acc = self.max_accel\n",
    "        action =  np.array([acc, d])\n",
    "#         print(f'Action acc {acc:.5f}, lane change {d}')\n",
    "        yield action\n",
    "\n",
    "sumo_params.render = False\n",
    "env = env_name(env_params, sumo_params, network)\n",
    "\n",
    "exp = FullExperiment(env)\n",
    "\n",
    "agent = RandomAgent(env)\n",
    "\n",
    "_ = exp.run(10, 3000, convert_to_csv=True, rl_actions=agent.act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0, return: 3740.6555759515295\n",
      "Round 1, return: 4115.766751729745\n",
      "Round 2, return: 2833.4711519168195\n",
      "Round 3, return: 2832.8820155768735\n",
      "Round 4, return: 3803.7717450343916\n",
      "Round 5, return: 3761.857234484111\n",
      "Round 6, return: 3727.966912218524\n",
      "Round 7, return: 3809.5319302063267\n",
      "Round 8, return: 3660.0931808848077\n",
      "Round 9, return: 2368.4143970948403\n",
      "Average, std return: 3465.4410895097963, 541.2198206627497\n",
      "Average, std speed: 6.517645961584906, 0.3936315772803137\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Best: max accel and change lane when speed is decreasing from last timestamp\n",
    "Reset PID when speed decreases and change lane (try to reach max velocity again)\n",
    "\n",
    "\n",
    "Max acc 2500 mean 600 std\n",
    "PID 2200 mean 400 std\n",
    "\n",
    "With steering\n",
    "Max acc 3600 mean 880 std\n",
    "PID 2600 mean 100 std\n",
    "\n",
    "Final\n",
    "Max accel Average, std return: 3495.8623019177276, 755.1837757284029\n",
    "PID Average, std return: 3823.135477809101, 503.36998899210744\n",
    "\n",
    "PID Second Run\n",
    "\n",
    "Round 0, return: 3740.6555759515295\n",
    "Round 1, return: 4115.766751729745\n",
    "Round 2, return: 2833.4711519168195\n",
    "Round 3, return: 2832.8820155768735\n",
    "Round 4, return: 3803.7717450343916\n",
    "Round 5, return: 3761.857234484111\n",
    "Round 6, return: 3727.966912218524\n",
    "Round 7, return: 3809.5319302063267\n",
    "Round 8, return: 3660.0931808848077\n",
    "Round 9, return: 2368.4143970948403\n",
    "Average, std return: 3465.4410895097963, 541.2198206627497\n",
    "Average, std speed: 6.517645961584906, 0.3936315772803137\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from flow.core.experiment_with_reward import FullExperiment\n",
    "import numpy as np\n",
    "\n",
    "KP = 10\n",
    "KI = 0.3\n",
    "KD = 100\n",
    "PAST_K = 30\n",
    "\n",
    "class PIDAgent():\n",
    "    def __init__(self, env):\n",
    "        self.action_space = env.action_space\n",
    "        self.max_decel = env.env_params.additional_params[\"max_decel\"]\n",
    "        self.max_accel = env.env_params.additional_params[\"max_accel\"]\n",
    "#         self.target_velocity = env.env_params.additional_params[\"target_velocity\"]\n",
    "        self.target_velocity = env.net_params.additional_params['speed_limit']\n",
    "\n",
    "#         print(env.net_params.additional_params)\n",
    "#         print(help(env.network))\n",
    "        self.max_speed = env.net_params.additional_params['speed_limit']\n",
    "        self.current_target_velocity = self.target_velocity\n",
    "        self.lanes = env.net_params.additional_params['lanes']\n",
    "        self.past_delta_velocities = []\n",
    "        self.Kp = KP\n",
    "        self.Ki = KI\n",
    "        self.Kd = KD\n",
    "        self.past_deltas = PAST_K\n",
    "        self.num_steps = 0\n",
    "        self.prev_speed = None\n",
    "        \n",
    "    def split_state(self, state):\n",
    "        return np.split(state, 3)\n",
    "    \n",
    "    def change_lane(self, current_lane):\n",
    "        if current_lane == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2\n",
    "    \n",
    "    def other_lane(self, current_lane):\n",
    "        if current_lane == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def nearby_vehicles_on_lane(self, target_lane, current_position, other_positions, other_lanes, nearby = 0.05):\n",
    "        for i in range(1, len(other_positions)):\n",
    "             if other_lanes[i] == target_lane:\n",
    "                if np.abs(other_positions[i] - current_position) < nearby:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def update_current_target_velocity(self, new_target):\n",
    "        if not np.allclose(new_target, self.current_target_velocity):\n",
    "            self.past_delta_velocities = []\n",
    "        self.current_target_velocity = new_target\n",
    "        \n",
    "    def act(self, state, reward, done, info):\n",
    "        self.num_steps += 1\n",
    "        speed, pos, lane = self.split_state(state)\n",
    "        current_speed = speed[0] * self.max_speed\n",
    "        lane = (np.round(self.lanes * lane)).astype(np.int)\n",
    "        current_lane = lane[0]\n",
    "        current_position = pos[0]\n",
    "        other_lane = self.other_lane(current_lane)\n",
    "        d = 1\n",
    "        if self.prev_speed is not None and self.prev_speed > current_speed:\n",
    "            if self.nearby_vehicles_on_lane(current_lane, current_position, pos[1:], lane[1:], 0.025):\n",
    "                if not self.nearby_vehicles_on_lane(other_lane, current_position, pos[1:], lane[1:], 0.025):\n",
    "                    d = self.change_lane(lane[0])\n",
    "                    self.update_current_target_velocity(self.target_velocity)\n",
    "                else:\n",
    "                    self.update_current_target_velocity(0)\n",
    "            else:\n",
    "                self.update_current_target_velocity(self.target_velocity)\n",
    "        else:\n",
    "            self.update_current_target_velocity(self.target_velocity)\n",
    "            \n",
    "        self.prev_speed = current_speed\n",
    "        \n",
    "        diff = self.current_target_velocity - current_speed\n",
    "        self.past_delta_velocities.append(diff)\n",
    "        self.past_delta_velocities = self.past_delta_velocities[- self.past_deltas:]\n",
    "        p = diff\n",
    "        i = sum(self.past_delta_velocities)\n",
    "        der = 0\n",
    "        if len(self.past_delta_velocities) > 2:\n",
    "            der = self.past_delta_velocities[-1] - self.past_delta_velocities[-2]\n",
    "        acc = self.Kp * p + self.Ki * i + self.Kd * der\n",
    "#         print(f\"{current_speed:.4f}/{self.current_target_velocity:.4f}, {p:.4f}, {len(self.past_delta_velocities)}, {i:.4f}, {der:.4f}, {reward:.4f}\")\n",
    "#         print(len(self.past_delta_velocities), self.past_delta_velocities[-1])\n",
    "        acc = max(min(acc, self.max_accel), - self.max_decel)\n",
    "#         acc = self.max_accel\n",
    "        action =  np.array([acc, d])\n",
    "        yield action\n",
    "\n",
    "sumo_params.render = False\n",
    "env = env_name(env_params, sumo_params, network)\n",
    "\n",
    "exp = FullExperiment(env)\n",
    "\n",
    "agent = PIDAgent(env)\n",
    "\n",
    "_ = exp.run(10, 3000, convert_to_csv=True, rl_actions=agent.act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0, return: 2465.5635179896326\n",
      "Round 1, return: 2804.5172637262854\n",
      "Round 2, return: 2926.9330722507043\n",
      "Round 3, return: 2636.5917181581663\n",
      "Round 4, return: 2834.5662420127655\n",
      "Round 5, return: 2938.824138425596\n",
      "Round 6, return: 2139.17540433827\n",
      "Round 7, return: 2124.280556513759\n",
      "Round 8, return: 2086.321631063388\n",
      "Round 9, return: 2554.2001131978577\n",
      "Average, std return: 2551.0973657676423, 319.1652396442611\n",
      "Average, std speed: 7.764650203414684, 0.17084722447599657\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ENV 2\n",
    "\n",
    "Round 0, return: 2465.5635179896326\n",
    "Round 1, return: 2804.5172637262854\n",
    "Round 2, return: 2926.9330722507043\n",
    "Round 3, return: 2636.5917181581663\n",
    "Round 4, return: 2834.5662420127655\n",
    "Round 5, return: 2938.824138425596\n",
    "Round 6, return: 2139.17540433827\n",
    "Round 7, return: 2124.280556513759\n",
    "Round 8, return: 2086.321631063388\n",
    "Round 9, return: 2554.2001131978577\n",
    "Average, std return: 2551.0973657676423, 319.1652396442611\n",
    "Average, std speed: 7.764650203414684, 0.17084722447599657\n",
    "'''\n",
    "\n",
    "\n",
    "from flow.core.experiment_with_reward import FullExperiment\n",
    "import numpy as np\n",
    "\n",
    "KP = 10\n",
    "KI = 0.3\n",
    "KD = 100\n",
    "PAST_K = 30\n",
    "\n",
    "class PIDAgent():\n",
    "    def __init__(self, env):\n",
    "        self.action_space = env.action_space\n",
    "        self.max_decel = env.env_params.additional_params[\"max_decel\"]\n",
    "        self.max_accel = env.env_params.additional_params[\"max_accel\"]\n",
    "#         self.target_velocity = env.env_params.additional_params[\"target_velocity\"]\n",
    "        self.target_velocity = env.net_params.additional_params['speed_limit']\n",
    "\n",
    "#         print(env.net_params.additional_params)\n",
    "#         print(help(env.network))\n",
    "        self.max_speed = env.net_params.additional_params['speed_limit']\n",
    "        self.current_target_velocity = self.target_velocity\n",
    "        self.lanes = env.net_params.additional_params['lanes']\n",
    "        self.past_delta_velocities = []\n",
    "        self.Kp = KP\n",
    "        self.Ki = KI\n",
    "        self.Kd = KD\n",
    "        self.past_deltas = PAST_K\n",
    "        self.num_steps = 0\n",
    "        self.prev_speed = None\n",
    "        \n",
    "    def split_state(self, state):\n",
    "        return np.split(state, 3)\n",
    "    \n",
    "    def change_lane(self, current_lane):\n",
    "        if current_lane == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2\n",
    "    \n",
    "    def other_lane(self, current_lane):\n",
    "        if current_lane == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def nearby_vehicles_on_lane(self, target_lane, current_position, other_positions, other_lanes, nearby = 0.05):\n",
    "        for i in range(1, len(other_positions)):\n",
    "             if other_lanes[i] == target_lane:\n",
    "                if np.abs(other_positions[i] - current_position) < nearby:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def update_current_target_velocity(self, new_target):\n",
    "        if not np.allclose(new_target, self.current_target_velocity):\n",
    "            self.past_delta_velocities = []\n",
    "        self.current_target_velocity = new_target\n",
    "        \n",
    "    def act(self, state, reward, done, info):\n",
    "        self.num_steps += 1\n",
    "        speed, pos, lane = self.split_state(state)\n",
    "        current_speed = speed[0] * self.max_speed\n",
    "        lane = (np.round(self.lanes * lane)).astype(np.int)\n",
    "        current_lane = lane[0]\n",
    "        current_position = pos[0]\n",
    "        other_lane = self.other_lane(current_lane)\n",
    "        d = 1\n",
    "        if self.prev_speed is not None and self.prev_speed > current_speed:\n",
    "            if self.nearby_vehicles_on_lane(current_lane, current_position, pos[1:], lane[1:], 0.025):\n",
    "                if not self.nearby_vehicles_on_lane(other_lane, current_position, pos[1:], lane[1:], 0.025):\n",
    "                    d = self.change_lane(lane[0])\n",
    "                    self.update_current_target_velocity(self.target_velocity)\n",
    "                else:\n",
    "                    self.update_current_target_velocity(0)\n",
    "            else:\n",
    "                self.update_current_target_velocity(self.target_velocity)\n",
    "        else:\n",
    "            self.update_current_target_velocity(self.target_velocity)\n",
    "            \n",
    "        self.prev_speed = current_speed\n",
    "        \n",
    "        diff = self.current_target_velocity - current_speed\n",
    "        self.past_delta_velocities.append(diff)\n",
    "        self.past_delta_velocities = self.past_delta_velocities[- self.past_deltas:]\n",
    "        p = diff\n",
    "        i = sum(self.past_delta_velocities)\n",
    "        der = 0\n",
    "        if len(self.past_delta_velocities) > 2:\n",
    "            der = self.past_delta_velocities[-1] - self.past_delta_velocities[-2]\n",
    "        acc = self.Kp * p + self.Ki * i + self.Kd * der\n",
    "#         print(f\"{current_speed:.4f}/{self.current_target_velocity:.4f}, {p:.4f}, {len(self.past_delta_velocities)}, {i:.4f}, {der:.4f}, {reward:.4f}\")\n",
    "#         print(len(self.past_delta_velocities), self.past_delta_velocities[-1])\n",
    "        acc = max(min(acc, self.max_accel), - self.max_decel)\n",
    "#         acc = self.max_accel\n",
    "        action =  np.array([acc, d])\n",
    "        yield action\n",
    "\n",
    "sumo_params.render = False\n",
    "env = env_name(env_params, sumo_params, network)\n",
    "\n",
    "exp = FullExperiment(env)\n",
    "\n",
    "agent = PIDAgent(env)\n",
    "\n",
    "_ = exp.run(10, 3000, convert_to_csv=True, rl_actions=agent.act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Feel free to experiment with all these problems and more!\n",
    "\n",
    "## Bibliography\n",
    "[1] Sugiyama, Yuki, et al. \"Traffic jams without bottlenecks—experimental evidence for the physical mechanism of the formation of a jam.\" New journal of physics 10.3 (2008): 033001.\n",
    "\n",
    "[2] Treiber, Martin, Ansgar Hennecke, and Dirk Helbing. \"Congested traffic states in empirical observations and microscopic simulations.\" Physical review E 62.2 (2000): 1805."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Setting up Flow Parameters\n",
    "\n",
    "RLlib experiments both generate a `params.json` file for each experiment run. For RLlib experiments, the parameters defining the Flow network and environment must be stored as well. As such, in this section we define the dictionary `flow_params`, which contains the variables required by the utility function `make_create_env`. `make_create_env` is a higher-order function which returns a function `create_env` that initializes a Gym environment corresponding to the Flow network specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "sumo_params.render = False\n",
    "sumo_params.print_warnings=False\n",
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag=name,\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name=env_name,\n",
    "    # name of the network class the experiment uses\n",
    "    network=network_name,\n",
    "    # simulator that is used by the experiment\n",
    "    simulator='traci',\n",
    "    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "    sim=sumo_params,\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=env_params,\n",
    "    # network-related parameters (see flow.core.params.NetParams and\n",
    "    # the network's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "    net=net_params,\n",
    "    # vehicles to be placed in the network at the start of a rollout \n",
    "    # (see flow.core.vehicles.Vehicles)\n",
    "    veh=vehicles,\n",
    "    # (optional) parameters affecting the positioning of vehicles upon \n",
    "    # initialization/reset (see flow.core.params.InitialConfig)\n",
    "    initial=initial_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Running RL experiments in Ray\n",
    "\n",
    "### 4.1 Import \n",
    "\n",
    "First, we must import modules required to run experiments in Ray. The `json` package is required to store the Flow experiment parameters in the `params.json` file, as is `FlowParamsEncoder`. Ray-related imports are required: the PPO algorithm agent, `ray.tune`'s experiment runner, and environment helper methods `register_env` and `make_create_env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Initializing Ray\n",
    "Here, we initialize Ray and experiment-based constant variables specifying parallelism in the experiment as well as experiment batch size in terms of number of rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-16 20:31:51,484\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-01-16_20-31-51_482993_4263/logs.\n",
      "2020-01-16 20:31:51,772\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:48810 to respond...\n",
      "2020-01-16 20:31:52,161\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:11375 to respond...\n",
      "2020-01-16 20:31:52,164\tINFO services.py:809 -- Starting Redis shard with 3.34 GB max memory.\n",
      "2020-01-16 20:31:52,217\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-01-16_20-31-51_482993_4263/logs.\n",
      "2020-01-16 20:31:52,219\tINFO services.py:1475 -- Starting the Plasma object store with 5.01 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.100',\n",
       " 'redis_address': '192.168.0.100:48810',\n",
       " 'object_store_address': '/tmp/ray/session_2020-01-16_20-31-51_482993_4263/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-01-16_20-31-51_482993_4263/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2020-01-16_20-31-51_482993_4263'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 4\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 20\n",
    "\n",
    "ray.init(num_cpus=N_CPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Configuration and Setup\n",
    "Here, we copy and modify the default configuration for the [PPO algorithm](https://arxiv.org/abs/1707.06347). The agent has the number of parallel workers specified, a batch size corresponding to `N_ROLLOUTS` rollouts (each of which has length `HORIZON` steps), a discount rate $\\gamma$ of 0.999, two hidden layers of size 16, uses Generalized Advantage Estimation, $\\lambda$ of 0.97, and other parameters as set below.\n",
    "\n",
    "Once `config` contains the desired parameters, a JSON string corresponding to the `flow_params` specified in section 3 is generated. The `FlowParamsEncoder` maps objects to string representations so that the experiment can be reproduced later. That string representation is stored within the `env_config` section of the `config` dictionary. Later, `config` is written out to the file `params.json`. \n",
    "\n",
    "Next, we call `make_create_env` and pass in the `flow_params` to return a function we can use to register our Flow environment with Gym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "HORIZON = 100\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS - 1  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [16, 16]})  # size of hidden layers in network\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 500  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Running Experiments\n",
    "\n",
    "Here, we use the `run_experiments` function from `ray.tune`. The function takes a dictionary with one key, a name corresponding to the experiment, and one value, itself a dictionary containing parameters for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-16 20:32:19,855\tINFO trial_runner.py:176 -- Starting a new experiment.\n",
      "2020-01-16 20:32:19,924\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2020-01-16 20:32:19,955\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 2.9/16.7 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-16 20:32:20,100\tWARNING util.py:145 -- The `start_trial` operation took 0.1779191493988037 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 2.9/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:32:22,411\tWARNING ppo.py:143 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:32:23,547\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:32:23.574914: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:32:24,120\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 69) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 69) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:32:25,660\tINFO rollout_worker.py:742 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f85cb26a3c8>}\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:32:25,660\tINFO rollout_worker.py:743 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f85cb26a080>}\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:32:25,661\tINFO rollout_worker.py:356 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f85cb50eef0>}\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:32:25,792\tINFO multi_gpu_optimizer.py:93 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m 2020-01-16 20:32:35,433\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m 2020-01-16 20:32:35.443454: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m 2020-01-16 20:32:35,507\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m 2020-01-16 20:32:35.574594: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m 2020-01-16 20:32:35,612\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m 2020-01-16 20:32:35.669683: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m 2020-01-16 20:32:36,255\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 69) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 69) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m WARNING:tensorflow:From /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:32:37,912\tINFO trainable.py:105 -- _setup took 16.070 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:32:37,912\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=5317)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m /home/victor/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=5318)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m 2020-01-16 20:32:41,801\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m 2020-01-16 20:32:43,188\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((69,), dtype=float64, min=0.0, max=0.967, mean=0.234)}}\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m 2020-01-16 20:32:43,188\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m 2020-01-16 20:32:43,188\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((69,), dtype=float64, min=0.0, max=0.967, mean=0.234)\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m 2020-01-16 20:32:43,188\tINFO sampler.py:407 -- Filtered obs: np.ndarray((69,), dtype=float64, min=0.0, max=0.967, mean=0.234)\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m 2020-01-16 20:32:43,189\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                                   'obs': np.ndarray((69,), dtype=float64, min=0.0, max=0.967, mean=0.234),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m 2020-01-16 20:32:43,189\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m 2020-01-16 20:32:43,805\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m { 'default_policy': ( { 'data': { 'batches': [ np.ndarray((1, 1), dtype=float32, min=0.501, max=0.501, mean=0.501),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                                                np.ndarray((1,), dtype=int64, min=2.0, max=2.0, mean=2.0)]},\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'type': 'TupleActions'},\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.117, max=0.117, mean=0.117),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'behaviour_logits': np.ndarray((1, 5), dtype=float32, min=-0.001, max=0.002, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.002, max=-0.002, mean=-0.002)})}\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m 2020-01-16 20:32:48,958\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((100,), dtype=float32, min=0.0, max=0.133, mean=0.092),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'actions': np.ndarray((100, 2), dtype=float32, min=-2.357, max=4.211, mean=0.52),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'advantages': np.ndarray((100,), dtype=float32, min=-10.265, max=0.058, mean=-4.888),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'agent_index': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'behaviour_logits': np.ndarray((100, 5), dtype=float32, min=-0.002, max=0.004, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'dones': np.ndarray((100,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'eps_id': np.ndarray((100,), dtype=int64, min=514873723.0, max=514873723.0, mean=514873723.0),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'infos': np.ndarray((100,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'new_obs': np.ndarray((100, 69), dtype=float32, min=0.0, max=0.981, mean=0.288),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'obs': np.ndarray((100, 69), dtype=float32, min=0.0, max=0.98, mean=0.287),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'prev_actions': np.ndarray((100, 2), dtype=float32, min=-2.357, max=4.211, mean=0.524),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'prev_rewards': np.ndarray((100,), dtype=float32, min=-1.0, max=0.121, mean=-0.23),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'rewards': np.ndarray((100,), dtype=float32, min=-1.0, max=0.121, mean=-0.229),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         't': np.ndarray((100,), dtype=int64, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'unroll_id': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'value_targets': np.ndarray((100,), dtype=float32, min=-10.266, max=0.056, mean=-4.89),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m                         'vf_preds': np.ndarray((100,), dtype=float32, min=-0.003, max=0.001, mean=-0.002)},\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m 2020-01-16 20:32:56,279\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.0, max=0.133, mean=0.094),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=-3.36, max=4.211, mean=0.499),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-19.436, max=0.078, mean=-6.792),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'behaviour_logits': np.ndarray((200, 5), dtype=float32, min=-0.005, max=0.005, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=514873723.0, max=1714897342.0, mean=1114885532.5),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'new_obs': np.ndarray((200, 69), dtype=float32, min=0.0, max=0.985, mean=0.269),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'obs': np.ndarray((200, 69), dtype=float32, min=0.0, max=0.985, mean=0.268),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=-3.36, max=4.211, mean=0.497),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=0.121, mean=-0.344),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=0.121, mean=-0.343),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=-19.438, max=0.074, mean=-6.794),\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=-0.005, max=0.001, mean=-0.002)},\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=5316)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,292\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc1/kernel:0' shape=(69, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,292\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc1/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,292\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc2/kernel:0' shape=(16, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,292\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc2/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,292\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc_out/kernel:0' shape=(16, 5) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,292\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc_out/bias:0' shape=(5,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,292\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc1/kernel:0' shape=(69, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,292\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc1/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,292\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc2/kernel:0' shape=(16, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,292\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc2/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,292\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc_out/kernel:0' shape=(16, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,293\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc_out/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,295\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m { 'inputs': [ np.ndarray((2000, 2), dtype=float32, min=-3.36, max=4.211, mean=0.504),\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m               np.ndarray((2000,), dtype=float32, min=-1.0, max=0.251, mean=-0.336),\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m               np.ndarray((2000, 69), dtype=float32, min=0.0, max=0.985, mean=0.258),\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m               np.ndarray((2000, 2), dtype=float32, min=-3.36, max=4.211, mean=0.51),\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m               np.ndarray((2000,), dtype=float32, min=-2.883, max=2.032, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m               np.ndarray((2000, 5), dtype=float32, min=-0.005, max=0.005, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m               np.ndarray((2000,), dtype=float32, min=-19.438, max=1.224, mean=-7.317),\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m               np.ndarray((2000,), dtype=float32, min=-0.005, max=0.004, mean=-0.001)],\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 69) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5315)\u001b[0m 2020-01-16 20:33:25,295\tINFO multi_gpu_impl.py:191 -- Divided 2000 rollout sequences, each of length 1, among 1 devices.\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-33-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: -20.44800047673816\n",
      "  episode_reward_mean: -33.91742584105496\n",
      "  episode_reward_min: -53.859783695635784\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 7553.225\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.496217966079712\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020019561052322388\n",
      "        policy_loss: -0.026747405529022217\n",
      "        total_loss: 64.89160919189453\n",
      "        vf_explained_var: -0.0008035898208618164\n",
      "        vf_loss: 64.91435241699219\n",
      "    load_time_ms: 54.565\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "    sample_time_ms: 44968.939\n",
      "    update_time_ms: 2220.718\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.26835443037974\n",
      "    ram_util_percent: 22.681012658227843\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.972448212905576\n",
      "    mean_inference_ms: 9.207705513879244\n",
      "    mean_processing_ms: 13.451381191911201\n",
      "  time_since_restore: 54.85447335243225\n",
      "  time_this_iter_s: 54.85447335243225\n",
      "  time_total_s: 54.85447335243225\n",
      "  timestamp: 1579199612\n",
      "  timesteps_since_restore: 2000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 1\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 54 s, 1 iter, 2000 ts, -33.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-34-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: -18.171150299662266\n",
      "  episode_reward_mean: -30.803461213139354\n",
      "  episode_reward_min: -53.859783695635784\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 40\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 6213.475\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.423642635345459\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.029850399121642113\n",
      "        policy_loss: -0.026176895946264267\n",
      "        total_loss: 39.59851837158203\n",
      "        vf_explained_var: -0.006167173385620117\n",
      "        vf_loss: 39.61874008178711\n",
      "    load_time_ms: 28.145\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    sample_time_ms: 44482.079\n",
      "    update_time_ms: 1115.07\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.97\n",
      "    ram_util_percent: 23.67428571428572\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 38.10556269353508\n",
      "    mean_inference_ms: 9.080336468597144\n",
      "    mean_processing_ms: 13.46385775951335\n",
      "  time_since_restore: 103.74560379981995\n",
      "  time_this_iter_s: 48.891130447387695\n",
      "  time_total_s: 103.74560379981995\n",
      "  timestamp: 1579199661\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 2\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.0/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 103 s, 2 iter, 4000 ts, -30.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-35-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: -4.451166007234357\n",
      "  episode_reward_mean: -26.80695611557661\n",
      "  episode_reward_min: -53.859783695635784\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 6139.637\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2872321605682373\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03138233348727226\n",
      "        policy_loss: -0.023131238296628\n",
      "        total_loss: 18.85207748413086\n",
      "        vf_explained_var: -0.003793001174926758\n",
      "        vf_loss: 18.868928909301758\n",
      "    load_time_ms: 19.577\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "    sample_time_ms: 43789.465\n",
      "    update_time_ms: 746.936\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.98695652173913\n",
      "    ram_util_percent: 23.73333333333333\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.92547475744164\n",
      "    mean_inference_ms: 9.044016949942279\n",
      "    mean_processing_ms: 13.468315288391684\n",
      "  time_since_restore: 152.1661057472229\n",
      "  time_this_iter_s: 48.420501947402954\n",
      "  time_total_s: 152.1661057472229\n",
      "  timestamp: 1579199710\n",
      "  timesteps_since_restore: 6000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 3\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.0/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 152 s, 3 iter, 6000 ts, -26.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-35-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 4.362917440140578\n",
      "  episode_reward_mean: -22.98805232077421\n",
      "  episode_reward_min: -53.859783695635784\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 80\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5936.042\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.132206916809082\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02644452638924122\n",
      "        policy_loss: -0.027806147933006287\n",
      "        total_loss: 8.063285827636719\n",
      "        vf_explained_var: -0.0032813549041748047\n",
      "        vf_loss: 8.08580207824707\n",
      "    load_time_ms: 15.113\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    sample_time_ms: 43079.58\n",
      "    update_time_ms: 563.185\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.948484848484846\n",
      "    ram_util_percent: 23.696969696969692\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.65813581110198\n",
      "    mean_inference_ms: 8.971240297281073\n",
      "    mean_processing_ms: 13.46524894811908\n",
      "  time_since_restore: 198.4680676460266\n",
      "  time_this_iter_s: 46.30196189880371\n",
      "  time_total_s: 198.4680676460266\n",
      "  timestamp: 1579199756\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 4\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.0/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 198 s, 4 iter, 8000 ts, -23 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-36-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 11.02337137339365\n",
      "  episode_reward_mean: -19.050176904145015\n",
      "  episode_reward_min: -53.859783695635784\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5899.371\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0058982372283936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0164419487118721\n",
      "        policy_loss: -0.02095605805516243\n",
      "        total_loss: 5.41771125793457\n",
      "        vf_explained_var: -0.004021644592285156\n",
      "        vf_loss: 5.4353790283203125\n",
      "    load_time_ms: 12.483\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "    sample_time_ms: 42789.321\n",
      "    update_time_ms: 453.62\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.24852941176471\n",
      "    ram_util_percent: 23.695588235294114\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.538469929090795\n",
      "    mean_inference_ms: 8.896675552411502\n",
      "    mean_processing_ms: 13.48117050769727\n",
      "  time_since_restore: 245.8775873184204\n",
      "  time_this_iter_s: 47.4095196723938\n",
      "  time_total_s: 245.8775873184204\n",
      "  timestamp: 1579199804\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 5\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.0/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 245 s, 5 iter, 10000 ts, -19.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-37-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 22.743562709108716\n",
      "  episode_reward_mean: -11.572027060220456\n",
      "  episode_reward_min: -42.469555043249734\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 120\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5733.675\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.896505355834961\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019530830904841423\n",
      "        policy_loss: -0.025966040790081024\n",
      "        total_loss: 7.911200046539307\n",
      "        vf_explained_var: -0.004057526588439941\n",
      "        vf_loss: 7.933260917663574\n",
      "    load_time_ms: 10.679\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    sample_time_ms: 42976.868\n",
      "    update_time_ms: 379.85\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.22173913043479\n",
      "    ram_util_percent: 23.655072463768114\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.36877780298709\n",
      "    mean_inference_ms: 8.764811088409974\n",
      "    mean_processing_ms: 13.499998679963493\n",
      "  time_since_restore: 294.72104716300964\n",
      "  time_this_iter_s: 48.84345984458923\n",
      "  time_total_s: 294.72104716300964\n",
      "  timestamp: 1579199852\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 6\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.0/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 294 s, 6 iter, 12000 ts, -11.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-38-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 22.743562709108716\n",
      "  episode_reward_mean: -3.7970722112394664\n",
      "  episode_reward_min: -29.823035638171437\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 140\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5720.347\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.77500581741333\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019310569390654564\n",
      "        policy_loss: -0.019296890124678612\n",
      "        total_loss: 14.27773666381836\n",
      "        vf_explained_var: -0.0057032108306884766\n",
      "        vf_loss: 14.293171882629395\n",
      "    load_time_ms: 9.382\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "    sample_time_ms: 42804.507\n",
      "    update_time_ms: 326.932\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.692647058823525\n",
      "    ram_util_percent: 23.722058823529416\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.1607776737836\n",
      "    mean_inference_ms: 8.66793202967711\n",
      "    mean_processing_ms: 13.519312601751349\n",
      "  time_since_restore: 342.1549596786499\n",
      "  time_this_iter_s: 47.43391251564026\n",
      "  time_total_s: 342.1549596786499\n",
      "  timestamp: 1579199900\n",
      "  timesteps_since_restore: 14000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 7\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.0/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 342 s, 7 iter, 14000 ts, -3.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-39-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 27.986742056211202\n",
      "  episode_reward_mean: 3.5321243144419565\n",
      "  episode_reward_min: -20.637598311599177\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 160\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5619.192\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6870200634002686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019506536424160004\n",
      "        policy_loss: -0.02216419018805027\n",
      "        total_loss: 25.382883071899414\n",
      "        vf_explained_var: 0.006186485290527344\n",
      "        vf_loss: 25.40113639831543\n",
      "    load_time_ms: 8.414\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    sample_time_ms: 42882.367\n",
      "    update_time_ms: 287.246\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.047826086956526\n",
      "    ram_util_percent: 23.608695652173903\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.08973485212942\n",
      "    mean_inference_ms: 8.564240282717872\n",
      "    mean_processing_ms: 13.544202809563904\n",
      "  time_since_restore: 390.51670145988464\n",
      "  time_this_iter_s: 48.36174178123474\n",
      "  time_total_s: 390.51670145988464\n",
      "  timestamp: 1579199948\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 8\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 390 s, 8 iter, 16000 ts, 3.53 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-39-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 39.379368524647816\n",
      "  episode_reward_mean: 11.114977389473914\n",
      "  episode_reward_min: -13.525869822043191\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 180\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5599.282\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6725131273269653\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006586547940969467\n",
      "        policy_loss: -0.012482646852731705\n",
      "        total_loss: 51.88948440551758\n",
      "        vf_explained_var: 0.004322171211242676\n",
      "        vf_loss: 51.90061569213867\n",
      "    load_time_ms: 7.975\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "    sample_time_ms: 42836.752\n",
      "    update_time_ms: 256.523\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.2220588235294\n",
      "    ram_util_percent: 23.50735294117647\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.109310355306995\n",
      "    mean_inference_ms: 8.508894589035215\n",
      "    mean_processing_ms: 13.574273344322282\n",
      "  time_since_restore: 438.45535945892334\n",
      "  time_this_iter_s: 47.938657999038696\n",
      "  time_total_s: 438.45535945892334\n",
      "  timestamp: 1579199996\n",
      "  timesteps_since_restore: 18000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 9\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 438 s, 9 iter, 18000 ts, 11.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-40-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 43.521395620296694\n",
      "  episode_reward_mean: 17.776418651558664\n",
      "  episode_reward_min: -6.890350037846828\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 200\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5579.256\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6099693775177002\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017068825662136078\n",
      "        policy_loss: -0.014761731959879398\n",
      "        total_loss: 64.45421600341797\n",
      "        vf_explained_var: 0.0016091465950012207\n",
      "        vf_loss: 64.46726989746094\n",
      "    load_time_ms: 7.382\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    sample_time_ms: 42865.645\n",
      "    update_time_ms: 231.833\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.79999999999999\n",
      "    ram_util_percent: 23.545714285714276\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.08963615058041\n",
      "    mean_inference_ms: 8.485169985405083\n",
      "    mean_processing_ms: 13.582521372831945\n",
      "  time_since_restore: 487.0027287006378\n",
      "  time_this_iter_s: 48.54736924171448\n",
      "  time_total_s: 487.0027287006378\n",
      "  timestamp: 1579200045\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 10\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 487 s, 10 iter, 20000 ts, 17.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-41-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 54.43859858645439\n",
      "  episode_reward_mean: 24.743281038988307\n",
      "  episode_reward_min: -0.29445887815919536\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 220\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5455.742\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5687265396118164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008738933131098747\n",
      "        policy_loss: -0.015648426488041878\n",
      "        total_loss: 104.36178588867188\n",
      "        vf_explained_var: -0.0003745555877685547\n",
      "        vf_loss: 104.37654113769531\n",
      "    load_time_ms: 2.089\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "    sample_time_ms: 43203.495\n",
      "    update_time_ms: 10.851\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.52948717948718\n",
      "    ram_util_percent: 24.646153846153844\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.22129216231508\n",
      "    mean_inference_ms: 8.456512357911583\n",
      "    mean_processing_ms: 13.604285851398986\n",
      "  time_since_restore: 541.6954064369202\n",
      "  time_this_iter_s: 54.69267773628235\n",
      "  time_total_s: 541.6954064369202\n",
      "  timestamp: 1579200100\n",
      "  timesteps_since_restore: 22000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 11\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 541 s, 11 iter, 22000 ts, 24.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-42-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 54.43859858645439\n",
      "  episode_reward_mean: 31.14078291355393\n",
      "  episode_reward_min: 8.178057105016402\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 240\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5565.044\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05000000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5274074077606201\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01048368588089943\n",
      "        policy_loss: -0.009240410290658474\n",
      "        total_loss: 124.15666961669922\n",
      "        vf_explained_var: -0.00012433528900146484\n",
      "        vf_loss: 124.16539001464844\n",
      "    load_time_ms: 2.282\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    sample_time_ms: 43587.857\n",
      "    update_time_ms: 11.378\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.13157894736842\n",
      "    ram_util_percent: 24.918421052631587\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.465795286285726\n",
      "    mean_inference_ms: 8.42114271957483\n",
      "    mean_processing_ms: 13.631655662440862\n",
      "  time_since_restore: 595.5361685752869\n",
      "  time_this_iter_s: 53.8407621383667\n",
      "  time_total_s: 595.5361685752869\n",
      "  timestamp: 1579200153\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 12\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 595 s, 12 iter, 24000 ts, 31.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-43-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 59.1441566754823\n",
      "  episode_reward_mean: 36.92121640245507\n",
      "  episode_reward_min: 15.753466245973971\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 260\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5504.274\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05000000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.470334529876709\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006596106104552746\n",
      "        policy_loss: -0.012580185197293758\n",
      "        total_loss: 147.18441772460938\n",
      "        vf_explained_var: -9.572505950927734e-05\n",
      "        vf_loss: 147.19659423828125\n",
      "    load_time_ms: 2.207\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "    sample_time_ms: 43911.311\n",
      "    update_time_ms: 11.768\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.84794520547945\n",
      "    ram_util_percent: 24.9082191780822\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.81745075381584\n",
      "    mean_inference_ms: 8.399105922854458\n",
      "    mean_processing_ms: 13.656051738752538\n",
      "  time_since_restore: 646.5887832641602\n",
      "  time_this_iter_s: 51.05261468887329\n",
      "  time_total_s: 646.5887832641602\n",
      "  timestamp: 1579200204\n",
      "  timesteps_since_restore: 26000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 13\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 646 s, 13 iter, 26000 ts, 36.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-44-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 59.1441566754823\n",
      "  episode_reward_mean: 40.90524577569874\n",
      "  episode_reward_min: 19.011588862862578\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 280\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5523.545\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.02500000037252903\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4779151678085327\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019070643931627274\n",
      "        policy_loss: -0.016537543386220932\n",
      "        total_loss: 142.05606079101562\n",
      "        vf_explained_var: -0.0001266002655029297\n",
      "        vf_loss: 142.07211303710938\n",
      "    load_time_ms: 2.206\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    sample_time_ms: 44002.699\n",
      "    update_time_ms: 12.011\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.733823529411765\n",
      "    ram_util_percent: 24.880882352941185\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 38.12037053473822\n",
      "    mean_inference_ms: 8.364493153382357\n",
      "    mean_processing_ms: 13.674451269970055\n",
      "  time_since_restore: 693.9978628158569\n",
      "  time_this_iter_s: 47.40907955169678\n",
      "  time_total_s: 693.9978628158569\n",
      "  timestamp: 1579200252\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 14\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 693 s, 14 iter, 28000 ts, 40.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-45-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 62.63436501733839\n",
      "  episode_reward_mean: 45.72526572692222\n",
      "  episode_reward_min: 22.633937989243115\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 300\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5490.417\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.02500000037252903\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4396675825119019\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012392675504088402\n",
      "        policy_loss: -0.01339428685605526\n",
      "        total_loss: 191.7281494140625\n",
      "        vf_explained_var: -2.86102294921875e-05\n",
      "        vf_loss: 191.74118041992188\n",
      "    load_time_ms: 2.242\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "    sample_time_ms: 44156.779\n",
      "    update_time_ms: 11.548\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.401449275362324\n",
      "    ram_util_percent: 25.002898550724634\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 38.46492014684056\n",
      "    mean_inference_ms: 8.33916577024946\n",
      "    mean_processing_ms: 13.70599375018241\n",
      "  time_since_restore: 742.6119735240936\n",
      "  time_this_iter_s: 48.614110708236694\n",
      "  time_total_s: 742.6119735240936\n",
      "  timestamp: 1579200301\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 15\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 742 s, 15 iter, 30000 ts, 45.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-45-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 73.0947760783397\n",
      "  episode_reward_mean: 49.1515668734215\n",
      "  episode_reward_min: 25.416217196260384\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 320\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5503.479\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.02500000037252903\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4864389896392822\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01063448004424572\n",
      "        policy_loss: -0.013660320080816746\n",
      "        total_loss: 200.7425994873047\n",
      "        vf_explained_var: -2.372264862060547e-05\n",
      "        vf_loss: 200.75596618652344\n",
      "    load_time_ms: 2.252\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    sample_time_ms: 44237.716\n",
      "    update_time_ms: 11.486\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.732394366197184\n",
      "    ram_util_percent: 25.030985915492945\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 38.65666876519689\n",
      "    mean_inference_ms: 8.329179319502723\n",
      "    mean_processing_ms: 13.724056086358406\n",
      "  time_since_restore: 792.3951554298401\n",
      "  time_this_iter_s: 49.78318190574646\n",
      "  time_total_s: 792.3951554298401\n",
      "  timestamp: 1579200350\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 16\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 792 s, 16 iter, 32000 ts, 49.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-46-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 73.0947760783397\n",
      "  episode_reward_mean: 52.19342168261514\n",
      "  episode_reward_min: 25.416217196260384\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 340\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5512.674\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.02500000037252903\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4015401601791382\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015352978371083736\n",
      "        policy_loss: -0.014545867219567299\n",
      "        total_loss: 217.14073181152344\n",
      "        vf_explained_var: -4.601478576660156e-05\n",
      "        vf_loss: 217.15493774414062\n",
      "    load_time_ms: 2.256\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "    sample_time_ms: 44193.009\n",
      "    update_time_ms: 11.63\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.86268656716418\n",
      "    ram_util_percent: 25.079104477611935\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 38.675552263184066\n",
      "    mean_inference_ms: 8.341083596191321\n",
      "    mean_processing_ms: 13.726054482705827\n",
      "  time_since_restore: 839.4748072624207\n",
      "  time_this_iter_s: 47.079651832580566\n",
      "  time_total_s: 839.4748072624207\n",
      "  timestamp: 1579200397\n",
      "  timesteps_since_restore: 34000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 17\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 839 s, 17 iter, 34000 ts, 52.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-47-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 74.77631652327686\n",
      "  episode_reward_mean: 53.62970369265674\n",
      "  episode_reward_min: 25.416217196260384\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 360\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5551.487\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.02500000037252903\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4229185581207275\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012142403051257133\n",
      "        policy_loss: -0.018353872001171112\n",
      "        total_loss: 185.04705810546875\n",
      "        vf_explained_var: -3.62396240234375e-05\n",
      "        vf_loss: 185.06507873535156\n",
      "    load_time_ms: 2.252\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    sample_time_ms: 44113.743\n",
      "    update_time_ms: 11.614\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.74202898550724\n",
      "    ram_util_percent: 25.18550724637681\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 38.556024181332695\n",
      "    mean_inference_ms: 8.326551591272429\n",
      "    mean_processing_ms: 13.725348165226928\n",
      "  time_since_restore: 887.4301414489746\n",
      "  time_this_iter_s: 47.955334186553955\n",
      "  time_total_s: 887.4301414489746\n",
      "  timestamp: 1579200445\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 18\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 887 s, 18 iter, 36000 ts, 53.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-48-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 75.14667575579261\n",
      "  episode_reward_mean: 55.96977811105135\n",
      "  episode_reward_min: 28.88803250683586\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 380\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5532.734\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.02500000037252903\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4140844345092773\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007536778226494789\n",
      "        policy_loss: -0.015009647235274315\n",
      "        total_loss: 212.9984130859375\n",
      "        vf_explained_var: -1.4185905456542969e-05\n",
      "        vf_loss: 213.0131378173828\n",
      "    load_time_ms: 1.989\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "    sample_time_ms: 43833.753\n",
      "    update_time_ms: 11.466\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.771875\n",
      "    ram_util_percent: 25.198437499999997\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 38.48918285345864\n",
      "    mean_inference_ms: 8.318505723140708\n",
      "    mean_processing_ms: 13.726262314210048\n",
      "  time_since_restore: 932.3806703090668\n",
      "  time_this_iter_s: 44.95052886009216\n",
      "  time_total_s: 932.3806703090668\n",
      "  timestamp: 1579200490\n",
      "  timesteps_since_restore: 38000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 19\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 932 s, 19 iter, 38000 ts, 56 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-48-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 75.25611703238239\n",
      "  episode_reward_mean: 56.76535283864826\n",
      "  episode_reward_min: 28.88803250683586\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 400\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5515.625\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.012500000186264515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4133974313735962\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025703759863972664\n",
      "        policy_loss: -0.029554743319749832\n",
      "        total_loss: 212.50140380859375\n",
      "        vf_explained_var: -1.3828277587890625e-05\n",
      "        vf_loss: 212.5307159423828\n",
      "    load_time_ms: 1.945\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    sample_time_ms: 43751.977\n",
      "    update_time_ms: 11.121\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.41764705882353\n",
      "    ram_util_percent: 25.241176470588236\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 38.42450132081487\n",
      "    mean_inference_ms: 8.304691053177361\n",
      "    mean_processing_ms: 13.720347403058106\n",
      "  time_since_restore: 979.9362406730652\n",
      "  time_this_iter_s: 47.55557036399841\n",
      "  time_total_s: 979.9362406730652\n",
      "  timestamp: 1579200538\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 20\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 979 s, 20 iter, 40000 ts, 56.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-49-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.05750454953599\n",
      "  episode_reward_mean: 57.009936822408406\n",
      "  episode_reward_min: 24.77815851313618\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 420\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5401.854\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.012500000186264515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.338766098022461\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01567266322672367\n",
      "        policy_loss: -0.020863071084022522\n",
      "        total_loss: 208.6938934326172\n",
      "        vf_explained_var: -1.5735626220703125e-05\n",
      "        vf_loss: 208.7146453857422\n",
      "    load_time_ms: 1.944\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "    sample_time_ms: 43104.7\n",
      "    update_time_ms: 11.185\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.37164179104479\n",
      "    ram_util_percent: 25.17313432835821\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 38.30124816542926\n",
      "    mean_inference_ms: 8.27428716695297\n",
      "    mean_processing_ms: 13.709942236217351\n",
      "  time_since_restore: 1027.0154037475586\n",
      "  time_this_iter_s: 47.07916307449341\n",
      "  time_total_s: 1027.0154037475586\n",
      "  timestamp: 1579200585\n",
      "  timesteps_since_restore: 42000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 21\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1027 s, 21 iter, 42000 ts, 57 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-50-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.05750454953599\n",
      "  episode_reward_mean: 57.82967896484639\n",
      "  episode_reward_min: 24.77815851313618\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 440\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5290.242\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.012500000186264515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3072998523712158\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01166773121803999\n",
      "        policy_loss: -0.01769258826971054\n",
      "        total_loss: 242.28131103515625\n",
      "        vf_explained_var: -4.172325134277344e-06\n",
      "        vf_loss: 242.29879760742188\n",
      "    load_time_ms: 1.742\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    sample_time_ms: 42232.971\n",
      "    update_time_ms: 10.612\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.64761904761905\n",
      "    ram_util_percent: 25.098412698412687\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 38.184399874513275\n",
      "    mean_inference_ms: 8.23976098871171\n",
      "    mean_processing_ms: 13.701549271858022\n",
      "  time_since_restore: 1071.010618686676\n",
      "  time_this_iter_s: 43.99521493911743\n",
      "  time_total_s: 1071.010618686676\n",
      "  timestamp: 1579200629\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 22\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1071 s, 22 iter, 44000 ts, 57.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-51-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.73658359786816\n",
      "  episode_reward_mean: 59.43543440169531\n",
      "  episode_reward_min: 24.77815851313618\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 460\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5291.084\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.012500000186264515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.391554832458496\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04769166186451912\n",
      "        policy_loss: -0.03664977476000786\n",
      "        total_loss: 243.67271423339844\n",
      "        vf_explained_var: 4.947185516357422e-06\n",
      "        vf_loss: 243.70875549316406\n",
      "    load_time_ms: 1.749\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "    sample_time_ms: 41947.622\n",
      "    update_time_ms: 10.055\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.59420289855074\n",
      "    ram_util_percent: 24.88695652173913\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 38.13392101133666\n",
      "    mean_inference_ms: 8.235555498454612\n",
      "    mean_processing_ms: 13.694152068036473\n",
      "  time_since_restore: 1119.210739850998\n",
      "  time_this_iter_s: 48.2001211643219\n",
      "  time_total_s: 1119.210739850998\n",
      "  timestamp: 1579200677\n",
      "  timesteps_since_restore: 46000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 23\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.1/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1119 s, 23 iter, 46000 ts, 59.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-52-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.73658359786816\n",
      "  episode_reward_mean: 60.22498318958792\n",
      "  episode_reward_min: 24.77815851313618\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 480\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5266.491\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.01875000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.342615008354187\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025640837848186493\n",
      "        policy_loss: -0.023310933262109756\n",
      "        total_loss: 235.6319122314453\n",
      "        vf_explained_var: -2.2649765014648438e-06\n",
      "        vf_loss: 235.65480041503906\n",
      "    load_time_ms: 1.748\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    sample_time_ms: 42108.788\n",
      "    update_time_ms: 9.714\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.07246376811594\n",
      "    ram_util_percent: 24.694202898550724\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 38.12494663994117\n",
      "    mean_inference_ms: 8.234465002743336\n",
      "    mean_processing_ms: 13.687348672710636\n",
      "  time_since_restore: 1167.9821894168854\n",
      "  time_this_iter_s: 48.77144956588745\n",
      "  time_total_s: 1167.9821894168854\n",
      "  timestamp: 1579200726\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 24\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.1/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1167 s, 24 iter, 48000 ts, 60.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-52-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.73658359786816\n",
      "  episode_reward_mean: 60.309612432685405\n",
      "  episode_reward_min: 24.77815851313618\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 500\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5213.668\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.01875000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3475040197372437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03373546525835991\n",
      "        policy_loss: -0.032223474234342575\n",
      "        total_loss: 209.2530059814453\n",
      "        vf_explained_var: -4.887580871582031e-06\n",
      "        vf_loss: 209.2846221923828\n",
      "    load_time_ms: 1.691\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "    sample_time_ms: 41919.24\n",
      "    update_time_ms: 9.575\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.46515151515151\n",
      "    ram_util_percent: 24.698484848484846\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 38.01261828754595\n",
      "    mean_inference_ms: 8.218143133575019\n",
      "    mean_processing_ms: 13.67239464970573\n",
      "  time_since_restore: 1214.1712725162506\n",
      "  time_this_iter_s: 46.189083099365234\n",
      "  time_total_s: 1214.1712725162506\n",
      "  timestamp: 1579200772\n",
      "  timesteps_since_restore: 50000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 25\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.1/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1214 s, 25 iter, 50000 ts, 60.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-53-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.73658359786816\n",
      "  episode_reward_mean: 60.11837247445813\n",
      "  episode_reward_min: -11.586318213759448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 520\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5213.267\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.01875000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.364562749862671\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024467460811138153\n",
      "        policy_loss: -0.03099299594759941\n",
      "        total_loss: 218.526123046875\n",
      "        vf_explained_var: 5.4836273193359375e-06\n",
      "        vf_loss: 218.556640625\n",
      "    load_time_ms: 1.688\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    sample_time_ms: 41264.307\n",
      "    update_time_ms: 9.619\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.71451612903226\n",
      "    ram_util_percent: 24.700000000000003\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.952914132074355\n",
      "    mean_inference_ms: 8.230066281749517\n",
      "    mean_processing_ms: 13.660638062501313\n",
      "  time_since_restore: 1257.4006006717682\n",
      "  time_this_iter_s: 43.22932815551758\n",
      "  time_total_s: 1257.4006006717682\n",
      "  timestamp: 1579200816\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 26\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.1/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1257 s, 26 iter, 52000 ts, 60.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-54-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.73658359786816\n",
      "  episode_reward_mean: 60.87441524792891\n",
      "  episode_reward_min: -11.586318213759448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 540\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5182.803\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.01875000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3224492073059082\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03823559731245041\n",
      "        policy_loss: -0.038341179490089417\n",
      "        total_loss: 270.4828186035156\n",
      "        vf_explained_var: -2.5033950805664062e-06\n",
      "        vf_loss: 270.5205078125\n",
      "    load_time_ms: 1.689\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "    sample_time_ms: 41156.087\n",
      "    update_time_ms: 9.534\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.643076923076926\n",
      "    ram_util_percent: 24.679999999999996\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.89865877033848\n",
      "    mean_inference_ms: 8.242439495770668\n",
      "    mean_processing_ms: 13.65188816143409\n",
      "  time_since_restore: 1303.0920059680939\n",
      "  time_this_iter_s: 45.691405296325684\n",
      "  time_total_s: 1303.0920059680939\n",
      "  timestamp: 1579200861\n",
      "  timesteps_since_restore: 54000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 27\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.1/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1303 s, 27 iter, 54000 ts, 60.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-55-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.7062328630673\n",
      "  episode_reward_mean: 59.381258554877334\n",
      "  episode_reward_min: -11.586318213759448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 560\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5132.961\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.01875000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3469367027282715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03716012090444565\n",
      "        policy_loss: -0.034367866814136505\n",
      "        total_loss: 187.827880859375\n",
      "        vf_explained_var: -1.430511474609375e-06\n",
      "        vf_loss: 187.86155700683594\n",
      "    load_time_ms: 1.724\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    sample_time_ms: 41185.323\n",
      "    update_time_ms: 9.634\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.56176470588235\n",
      "    ram_util_percent: 24.685294117647057\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.79290725542519\n",
      "    mean_inference_ms: 8.24391687899046\n",
      "    mean_processing_ms: 13.643396180556097\n",
      "  time_since_restore: 1350.8431186676025\n",
      "  time_this_iter_s: 47.75111269950867\n",
      "  time_total_s: 1350.8431186676025\n",
      "  timestamp: 1579200909\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 28\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.1/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1350 s, 28 iter, 56000 ts, 59.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-56-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.7062328630673\n",
      "  episode_reward_mean: 58.71361336988626\n",
      "  episode_reward_min: -11.586318213759448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 580\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5207.209\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.01875000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3899158239364624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0859149619936943\n",
      "        policy_loss: -0.056062523275613785\n",
      "        total_loss: 206.17160034179688\n",
      "        vf_explained_var: -3.0994415283203125e-06\n",
      "        vf_loss: 206.22604370117188\n",
      "    load_time_ms: 1.735\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "    sample_time_ms: 41750.302\n",
      "    update_time_ms: 9.72\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.26301369863013\n",
      "    ram_util_percent: 25.08082191780822\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.729312128538226\n",
      "    mean_inference_ms: 8.24433262606696\n",
      "    mean_processing_ms: 13.63847131876587\n",
      "  time_since_restore: 1402.1853308677673\n",
      "  time_this_iter_s: 51.342212200164795\n",
      "  time_total_s: 1402.1853308677673\n",
      "  timestamp: 1579200960\n",
      "  timesteps_since_restore: 58000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 29\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1402 s, 29 iter, 58000 ts, 58.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-56-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.23440965177761\n",
      "  episode_reward_mean: 58.28118906739851\n",
      "  episode_reward_min: -11.586318213759448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 600\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5222.131\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.02812499925494194\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4052284955978394\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.041114576160907745\n",
      "        policy_loss: -0.030638664960861206\n",
      "        total_loss: 209.62461853027344\n",
      "        vf_explained_var: -2.86102294921875e-06\n",
      "        vf_loss: 209.6541290283203\n",
      "    load_time_ms: 1.735\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    sample_time_ms: 41948.294\n",
      "    update_time_ms: 10.301\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.28450704225352\n",
      "    ram_util_percent: 25.877464788732393\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.706318462802464\n",
      "    mean_inference_ms: 8.241958060955023\n",
      "    mean_processing_ms: 13.640294025141179\n",
      "  time_since_restore: 1451.8747458457947\n",
      "  time_this_iter_s: 49.689414978027344\n",
      "  time_total_s: 1451.8747458457947\n",
      "  timestamp: 1579201010\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 30\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1451 s, 30 iter, 60000 ts, 58.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-57-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.62755900320386\n",
      "  episode_reward_mean: 59.98062961693122\n",
      "  episode_reward_min: 3.231227868397771\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 620\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5186.683\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.04218750074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3957473039627075\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025565795600414276\n",
      "        policy_loss: -0.024274177849292755\n",
      "        total_loss: 270.8033142089844\n",
      "        vf_explained_var: -1.0728836059570312e-06\n",
      "        vf_loss: 270.8265380859375\n",
      "    load_time_ms: 1.744\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "    sample_time_ms: 41872.741\n",
      "    update_time_ms: 10.182\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.343939393939394\n",
      "    ram_util_percent: 26.060606060606055\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.69841073937202\n",
      "    mean_inference_ms: 8.231795161309032\n",
      "    mean_processing_ms: 13.643636493394219\n",
      "  time_since_restore: 1497.843177318573\n",
      "  time_this_iter_s: 45.96843147277832\n",
      "  time_total_s: 1497.843177318573\n",
      "  timestamp: 1579201056\n",
      "  timesteps_since_restore: 62000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 31\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.4/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1497 s, 31 iter, 62000 ts, 60 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-58-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.62755900320386\n",
      "  episode_reward_mean: 58.309634628571686\n",
      "  episode_reward_min: 3.231227868397771\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 640\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5237.669\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.04218750074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3005578517913818\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020420769229531288\n",
      "        policy_loss: -0.028868677094578743\n",
      "        total_loss: 227.9971923828125\n",
      "        vf_explained_var: -2.384185791015625e-07\n",
      "        vf_loss: 228.0252227783203\n",
      "    load_time_ms: 1.745\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    sample_time_ms: 41863.517\n",
      "    update_time_ms: 10.365\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.930158730158734\n",
      "    ram_util_percent: 25.939682539682543\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.672445687618556\n",
      "    mean_inference_ms: 8.215587873073325\n",
      "    mean_processing_ms: 13.64445625312415\n",
      "  time_since_restore: 1542.2564618587494\n",
      "  time_this_iter_s: 44.41328454017639\n",
      "  time_total_s: 1542.2564618587494\n",
      "  timestamp: 1579201101\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 32\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1542 s, 32 iter, 64000 ts, 58.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-59-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.62755900320386\n",
      "  episode_reward_mean: 59.70619906572282\n",
      "  episode_reward_min: 3.231227868397771\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 660\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5178.962\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.04218750074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3242920637130737\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03973153606057167\n",
      "        policy_loss: -0.03294805437326431\n",
      "        total_loss: 236.5231475830078\n",
      "        vf_explained_var: -5.960464477539062e-07\n",
      "        vf_loss: 236.55430603027344\n",
      "    load_time_ms: 1.728\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "    sample_time_ms: 41465.567\n",
      "    update_time_ms: 10.654\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.158064516129034\n",
      "    ram_util_percent: 25.919354838709683\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.6334649054611\n",
      "    mean_inference_ms: 8.194497830776525\n",
      "    mean_processing_ms: 13.642812183599167\n",
      "  time_since_restore: 1585.892462015152\n",
      "  time_this_iter_s: 43.63600015640259\n",
      "  time_total_s: 1585.892462015152\n",
      "  timestamp: 1579201144\n",
      "  timesteps_since_restore: 66000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 33\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1585 s, 33 iter, 66000 ts, 59.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_20-59-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.05392511472398\n",
      "  episode_reward_mean: 59.95414656585474\n",
      "  episode_reward_min: 3.231227868397771\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 680\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5152.392\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.04218750074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4881523847579956\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0378512404859066\n",
      "        policy_loss: -0.03772858530282974\n",
      "        total_loss: 213.67715454101562\n",
      "        vf_explained_var: -3.5762786865234375e-07\n",
      "        vf_loss: 213.7133026123047\n",
      "    load_time_ms: 1.734\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    sample_time_ms: 41170.905\n",
      "    update_time_ms: 10.51\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.049230769230768\n",
      "    ram_util_percent: 25.829230769230776\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.53634976346014\n",
      "    mean_inference_ms: 8.176271645314507\n",
      "    mean_processing_ms: 13.637945082462647\n",
      "  time_since_restore: 1631.4518086910248\n",
      "  time_this_iter_s: 45.5593466758728\n",
      "  time_total_s: 1631.4518086910248\n",
      "  timestamp: 1579201190\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 34\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1631 s, 34 iter, 68000 ts, 60 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-00-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 93.3140398100388\n",
      "  episode_reward_mean: 60.02809536698181\n",
      "  episode_reward_min: 3.231227868397771\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 700\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5164.679\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.04218750074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4940216541290283\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.06762565672397614\n",
      "        policy_loss: -0.04299142584204674\n",
      "        total_loss: 209.65460205078125\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 209.69476318359375\n",
      "    load_time_ms: 1.765\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "    sample_time_ms: 41316.44\n",
      "    update_time_ms: 10.67\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.47101449275362\n",
      "    ram_util_percent: 25.805797101449276\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.50527003886829\n",
      "    mean_inference_ms: 8.186138365764345\n",
      "    mean_processing_ms: 13.633818475114344\n",
      "  time_since_restore: 1679.2206718921661\n",
      "  time_this_iter_s: 47.76886320114136\n",
      "  time_total_s: 1679.2206718921661\n",
      "  timestamp: 1579201238\n",
      "  timesteps_since_restore: 70000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 35\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1679 s, 35 iter, 70000 ts, 60 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-01-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 96.08039112230053\n",
      "  episode_reward_mean: 59.49513745610277\n",
      "  episode_reward_min: 4.835248956406534\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 720\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5171.072\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.06328125298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.507682204246521\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.030470360070466995\n",
      "        policy_loss: -0.029609309509396553\n",
      "        total_loss: 236.71324157714844\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 236.7409210205078\n",
      "    load_time_ms: 1.761\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    sample_time_ms: 41482.996\n",
      "    update_time_ms: 10.501\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.31875\n",
      "    ram_util_percent: 25.809375000000003\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.39079555756048\n",
      "    mean_inference_ms: 8.175661137552096\n",
      "    mean_processing_ms: 13.627348408934074\n",
      "  time_since_restore: 1724.1791915893555\n",
      "  time_this_iter_s: 44.95851969718933\n",
      "  time_total_s: 1724.1791915893555\n",
      "  timestamp: 1579201283\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 36\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1724 s, 36 iter, 72000 ts, 59.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-02-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 99.30241019392702\n",
      "  episode_reward_mean: 60.52600602316969\n",
      "  episode_reward_min: 12.37564257612806\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 740\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5167.559\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.06328125298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2817755937576294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03170882165431976\n",
      "        policy_loss: -0.03459466993808746\n",
      "        total_loss: 249.92950439453125\n",
      "        vf_explained_var: -3.5762786865234375e-07\n",
      "        vf_loss: 249.96209716796875\n",
      "    load_time_ms: 1.771\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "    sample_time_ms: 41600.908\n",
      "    update_time_ms: 10.539\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.407462686567165\n",
      "    ram_util_percent: 25.8\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.362321526689286\n",
      "    mean_inference_ms: 8.191107933478868\n",
      "    mean_processing_ms: 13.622007666204762\n",
      "  time_since_restore: 1771.0162825584412\n",
      "  time_this_iter_s: 46.83709096908569\n",
      "  time_total_s: 1771.0162825584412\n",
      "  timestamp: 1579201329\n",
      "  timesteps_since_restore: 74000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 37\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1771 s, 37 iter, 74000 ts, 60.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-02-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 99.30241019392702\n",
      "  episode_reward_mean: 61.57101666230552\n",
      "  episode_reward_min: 12.37564257612806\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 760\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5235.322\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.06328125298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2608091831207275\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04329964518547058\n",
      "        policy_loss: -0.033887580037117004\n",
      "        total_loss: 286.0920104980469\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 286.1230163574219\n",
      "    load_time_ms: 1.735\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    sample_time_ms: 41593.022\n",
      "    update_time_ms: 10.572\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.63088235294117\n",
      "    ram_util_percent: 25.801470588235293\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.31071005410685\n",
      "    mean_inference_ms: 8.198030643319647\n",
      "    mean_processing_ms: 13.615544958367469\n",
      "  time_since_restore: 1819.3662807941437\n",
      "  time_this_iter_s: 48.349998235702515\n",
      "  time_total_s: 1819.3662807941437\n",
      "  timestamp: 1579201378\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 38\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1819 s, 38 iter, 76000 ts, 61.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 99.30241019392702\n",
      "  episode_reward_mean: 60.907402220445746\n",
      "  episode_reward_min: -0.27111140731894734\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 780\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5215.653\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09492187201976776\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5622421503067017\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0464625284075737\n",
      "        policy_loss: -0.036528024822473526\n",
      "        total_loss: 213.00411987304688\n",
      "        vf_explained_var: -2.384185791015625e-07\n",
      "        vf_loss: 213.03631591796875\n",
      "    load_time_ms: 1.734\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "    sample_time_ms: 41023.284\n",
      "    update_time_ms: 10.494\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.92769230769231\n",
      "    ram_util_percent: 25.80615384615385\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.26706571379032\n",
      "    mean_inference_ms: 8.198297707005784\n",
      "    mean_processing_ms: 13.61147915204595\n",
      "  time_since_restore: 1864.813110589981\n",
      "  time_this_iter_s: 45.4468297958374\n",
      "  time_total_s: 1864.813110589981\n",
      "  timestamp: 1579201423\n",
      "  timesteps_since_restore: 78000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 39\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1864 s, 39 iter, 78000 ts, 60.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-04-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 99.30241019392702\n",
      "  episode_reward_mean: 61.08247447299681\n",
      "  episode_reward_min: -16.8395598689468\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 800\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5182.924\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.14238281548023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4668270349502563\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04726569727063179\n",
      "        policy_loss: -0.04125775769352913\n",
      "        total_loss: 223.844970703125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 223.8795166015625\n",
      "    load_time_ms: 1.738\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    sample_time_ms: 40880.956\n",
      "    update_time_ms: 10.342\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.304347826086964\n",
      "    ram_util_percent: 25.889855072463774\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.170604476638644\n",
      "    mean_inference_ms: 8.187363701658287\n",
      "    mean_processing_ms: 13.606466086712421\n",
      "  time_since_restore: 1912.7506692409515\n",
      "  time_this_iter_s: 47.93755865097046\n",
      "  time_total_s: 1912.7506692409515\n",
      "  timestamp: 1579201471\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 40\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1912 s, 40 iter, 80000 ts, 61.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-05-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 101.23023545039187\n",
      "  episode_reward_mean: 60.3527298143807\n",
      "  episode_reward_min: -16.8395598689468\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 820\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5231.332\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.21357421576976776\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4354145526885986\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03646964579820633\n",
      "        policy_loss: -0.03624334558844566\n",
      "        total_loss: 233.12530517578125\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 233.15379333496094\n",
      "    load_time_ms: 1.73\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "    sample_time_ms: 41110.318\n",
      "    update_time_ms: 10.313\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.22028985507246\n",
      "    ram_util_percent: 25.891304347826093\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.18729908865129\n",
      "    mean_inference_ms: 8.206067469936649\n",
      "    mean_processing_ms: 13.60537248298131\n",
      "  time_since_restore: 1961.4966359138489\n",
      "  time_this_iter_s: 48.74596667289734\n",
      "  time_total_s: 1961.4966359138489\n",
      "  timestamp: 1579201520\n",
      "  timesteps_since_restore: 82000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 41\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 1961 s, 41 iter, 82000 ts, 60.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-06-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 101.23023545039187\n",
      "  episode_reward_mean: 59.215781859311164\n",
      "  episode_reward_min: -16.8395598689468\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 840\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5196.961\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.21357421576976776\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4048104286193848\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.044347092509269714\n",
      "        policy_loss: -0.047837864607572556\n",
      "        total_loss: 215.4322052001953\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 215.4706573486328\n",
      "    load_time_ms: 1.726\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    sample_time_ms: 41448.698\n",
      "    update_time_ms: 10.228\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.726470588235294\n",
      "    ram_util_percent: 25.88970588235295\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.12293172493764\n",
      "    mean_inference_ms: 8.197692968693358\n",
      "    mean_processing_ms: 13.603410970059347\n",
      "  time_since_restore: 2008.9484477043152\n",
      "  time_this_iter_s: 47.45181179046631\n",
      "  time_total_s: 2008.9484477043152\n",
      "  timestamp: 1579201567\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 42\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2008 s, 42 iter, 84000 ts, 59.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-06-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 101.23023545039187\n",
      "  episode_reward_mean: 56.93328180329033\n",
      "  episode_reward_min: -16.8395598689468\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 860\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5232.219\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4916090965270996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0224568173289299\n",
      "        policy_loss: -0.03819379210472107\n",
      "        total_loss: 182.18397521972656\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 182.2149658203125\n",
      "    load_time_ms: 1.735\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "    sample_time_ms: 41969.827\n",
      "    update_time_ms: 10.035\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.665714285714294\n",
      "    ram_util_percent: 25.934285714285714\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.170108991392304\n",
      "    mean_inference_ms: 8.213139894448815\n",
      "    mean_processing_ms: 13.60584723927304\n",
      "  time_since_restore: 2058.148274421692\n",
      "  time_this_iter_s: 49.19982671737671\n",
      "  time_total_s: 2058.148274421692\n",
      "  timestamp: 1579201617\n",
      "  timesteps_since_restore: 86000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 43\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2058 s, 43 iter, 86000 ts, 56.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-07-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 101.23023545039187\n",
      "  episode_reward_mean: 58.879893414315\n",
      "  episode_reward_min: -16.8395598689468\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 880\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5221.692\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.454732060432434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020386867225170135\n",
      "        policy_loss: -0.033004604279994965\n",
      "        total_loss: 249.60191345214844\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 249.6284942626953\n",
      "    load_time_ms: 1.724\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    sample_time_ms: 42164.033\n",
      "    update_time_ms: 10.335\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.654411764705884\n",
      "    ram_util_percent: 25.901470588235302\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.2257496487723\n",
      "    mean_inference_ms: 8.231491994764683\n",
      "    mean_processing_ms: 13.607325421022878\n",
      "  time_since_restore: 2105.5462231636047\n",
      "  time_this_iter_s: 47.39794874191284\n",
      "  time_total_s: 2105.5462231636047\n",
      "  timestamp: 1579201664\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 44\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2105 s, 44 iter, 88000 ts, 58.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-08-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 101.23023545039187\n",
      "  episode_reward_mean: 58.91455372897654\n",
      "  episode_reward_min: -20.850452035337163\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 900\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5241.456\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4447182416915894\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0212550088763237\n",
      "        policy_loss: -0.0322098471224308\n",
      "        total_loss: 262.2410583496094\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 262.2663879394531\n",
      "    load_time_ms: 1.702\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "    sample_time_ms: 41740.426\n",
      "    update_time_ms: 10.209\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.777419354838706\n",
      "    ram_util_percent: 25.888709677419367\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.218131792640264\n",
      "    mean_inference_ms: 8.23287738450003\n",
      "    mean_processing_ms: 13.607526808071432\n",
      "  time_since_restore: 2149.2763471603394\n",
      "  time_this_iter_s: 43.73012399673462\n",
      "  time_total_s: 2149.2763471603394\n",
      "  timestamp: 1579201708\n",
      "  timesteps_since_restore: 90000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 45\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2149 s, 45 iter, 90000 ts, 58.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-09-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 100.48193284779259\n",
      "  episode_reward_mean: 58.30415004975942\n",
      "  episode_reward_min: -20.850452035337163\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 920\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5247.435\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4820137023925781\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023864421993494034\n",
      "        policy_loss: -0.038292717188596725\n",
      "        total_loss: 186.2441864013672\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 186.27479553222656\n",
      "    load_time_ms: 1.725\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    sample_time_ms: 41894.328\n",
      "    update_time_ms: 10.606\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.89104477611941\n",
      "    ram_util_percent: 25.900000000000006\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.15840728336782\n",
      "    mean_inference_ms: 8.219011577805864\n",
      "    mean_processing_ms: 13.60455259211668\n",
      "  time_since_restore: 2195.8393335342407\n",
      "  time_this_iter_s: 46.56298637390137\n",
      "  time_total_s: 2195.8393335342407\n",
      "  timestamp: 1579201754\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 46\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2195 s, 46 iter, 92000 ts, 58.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-10-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 100.48193284779259\n",
      "  episode_reward_mean: 57.39475781107555\n",
      "  episode_reward_min: -30.169083637867026\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 940\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5197.549\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6992844343185425\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019089145585894585\n",
      "        policy_loss: -0.03537049889564514\n",
      "        total_loss: 195.43350219726562\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 195.46278381347656\n",
      "    load_time_ms: 1.716\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "    sample_time_ms: 41960.258\n",
      "    update_time_ms: 10.989\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.33731343283583\n",
      "    ram_util_percent: 25.891044776119408\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.142264305312075\n",
      "    mean_inference_ms: 8.217347671570067\n",
      "    mean_processing_ms: 13.603259783058672\n",
      "  time_since_restore: 2242.839412212372\n",
      "  time_this_iter_s: 47.0000786781311\n",
      "  time_total_s: 2242.839412212372\n",
      "  timestamp: 1579201801\n",
      "  timesteps_since_restore: 94000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 47\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2242 s, 47 iter, 94000 ts, 57.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-10-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 100.48193284779259\n",
      "  episode_reward_mean: 57.557309666753866\n",
      "  episode_reward_min: -30.169083637867026\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 960\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5185.446\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.470908284187317\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.034940894693136215\n",
      "        policy_loss: -0.042516689747571945\n",
      "        total_loss: 199.15306091308594\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 199.18435668945312\n",
      "    load_time_ms: 1.85\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    sample_time_ms: 41954.459\n",
      "    update_time_ms: 10.799\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.80441176470589\n",
      "    ram_util_percent: 25.84264705882353\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.11692409454292\n",
      "    mean_inference_ms: 8.215528403884123\n",
      "    mean_processing_ms: 13.600435431139253\n",
      "  time_since_restore: 2291.0104842185974\n",
      "  time_this_iter_s: 48.171072006225586\n",
      "  time_total_s: 2291.0104842185974\n",
      "  timestamp: 1579201850\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 48\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2291 s, 48 iter, 96000 ts, 57.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-11-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 100.48193284779259\n",
      "  episode_reward_mean: 55.41501746780084\n",
      "  episode_reward_min: -30.169083637867026\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 980\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5182.787\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6121745109558105\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022414684295654297\n",
      "        policy_loss: -0.03523335978388786\n",
      "        total_loss: 225.26272583007812\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 225.29086303710938\n",
      "    load_time_ms: 1.821\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "    sample_time_ms: 42078.335\n",
      "    update_time_ms: 10.738\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.41492537313434\n",
      "    ram_util_percent: 25.849253731343286\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.082477993385275\n",
      "    mean_inference_ms: 8.214527018593593\n",
      "    mean_processing_ms: 13.597956212622877\n",
      "  time_since_restore: 2337.6720130443573\n",
      "  time_this_iter_s: 46.66152882575989\n",
      "  time_total_s: 2337.6720130443573\n",
      "  timestamp: 1579201896\n",
      "  timesteps_since_restore: 98000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 49\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2337 s, 49 iter, 98000 ts, 55.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-12-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 93.74401683037094\n",
      "  episode_reward_mean: 54.271908574173125\n",
      "  episode_reward_min: -30.169083637867026\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1000\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5157.825\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4148629903793335\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023664576932787895\n",
      "        policy_loss: -0.03334323316812515\n",
      "        total_loss: 191.21725463867188\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 191.2429962158203\n",
      "    load_time_ms: 1.82\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    sample_time_ms: 41797.694\n",
      "    update_time_ms: 11.072\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.5453125\n",
      "    ram_util_percent: 25.789062500000007\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.10488325596352\n",
      "    mean_inference_ms: 8.224271681171011\n",
      "    mean_processing_ms: 13.594836685763692\n",
      "  time_since_restore: 2382.557326078415\n",
      "  time_this_iter_s: 44.88531303405762\n",
      "  time_total_s: 2382.557326078415\n",
      "  timestamp: 1579201941\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 50\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2382 s, 50 iter, 100000 ts, 54.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-13-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 93.74401683037094\n",
      "  episode_reward_mean: 53.781458877198936\n",
      "  episode_reward_min: -30.169083637867026\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1020\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5147.567\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.536041259765625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019388778135180473\n",
      "        policy_loss: -0.030071498826146126\n",
      "        total_loss: 187.62240600585938\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 187.646240234375\n",
      "    load_time_ms: 1.824\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "    sample_time_ms: 41722.937\n",
      "    update_time_ms: 11.196\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.294117647058826\n",
      "    ram_util_percent: 25.791176470588237\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.130919792443805\n",
      "    mean_inference_ms: 8.240886346464167\n",
      "    mean_processing_ms: 13.595119744485139\n",
      "  time_since_restore: 2430.453669309616\n",
      "  time_this_iter_s: 47.89634323120117\n",
      "  time_total_s: 2430.453669309616\n",
      "  timestamp: 1579201989\n",
      "  timesteps_since_restore: 102000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 51\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2430 s, 51 iter, 102000 ts, 53.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-13-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 95.69255521241855\n",
      "  episode_reward_mean: 54.50795765775184\n",
      "  episode_reward_min: -8.870019262866503\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1040\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5180.422\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.478014349937439\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015684034675359726\n",
      "        policy_loss: -0.024626046419143677\n",
      "        total_loss: 229.02853393554688\n",
      "        vf_explained_var: 1.7881393432617188e-07\n",
      "        vf_loss: 229.04812622070312\n",
      "    load_time_ms: 1.827\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    sample_time_ms: 41569.843\n",
      "    update_time_ms: 11.203\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.1\n",
      "    ram_util_percent: 25.800000000000004\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.146586334647125\n",
      "    mean_inference_ms: 8.257758567077829\n",
      "    mean_processing_ms: 13.594816602961899\n",
      "  time_since_restore: 2476.7037675380707\n",
      "  time_this_iter_s: 46.25009822845459\n",
      "  time_total_s: 2476.7037675380707\n",
      "  timestamp: 1579202035\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 52\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2476 s, 52 iter, 104000 ts, 54.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-14-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 95.69255521241855\n",
      "  episode_reward_mean: 55.16176851039615\n",
      "  episode_reward_min: -8.870019262866503\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1060\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5155.744\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4545396566390991\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02312295325100422\n",
      "        policy_loss: -0.03010648675262928\n",
      "        total_loss: 246.3709716796875\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 246.3936309814453\n",
      "    load_time_ms: 1.83\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "    sample_time_ms: 41110.764\n",
      "    update_time_ms: 11.205\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.65625\n",
      "    ram_util_percent: 25.806250000000006\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 37.05163842499809\n",
      "    mean_inference_ms: 8.250799099275499\n",
      "    mean_processing_ms: 13.591408637296727\n",
      "  time_since_restore: 2521.064120531082\n",
      "  time_this_iter_s: 44.360352993011475\n",
      "  time_total_s: 2521.064120531082\n",
      "  timestamp: 1579202080\n",
      "  timesteps_since_restore: 106000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 53\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2521 s, 53 iter, 106000 ts, 55.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-15-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 100.52111805444753\n",
      "  episode_reward_mean: 55.63567641940924\n",
      "  episode_reward_min: -17.050837169789244\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1080\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5181.495\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3087807893753052\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028609544038772583\n",
      "        policy_loss: -0.03965472802519798\n",
      "        total_loss: 243.7100830078125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 243.7406005859375\n",
      "    load_time_ms: 1.927\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    sample_time_ms: 40783.748\n",
      "    update_time_ms: 10.999\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.48888888888889\n",
      "    ram_util_percent: 25.799999999999997\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.998365662403\n",
      "    mean_inference_ms: 8.254702461376105\n",
      "    mean_processing_ms: 13.588665599657249\n",
      "  time_since_restore: 2565.447763442993\n",
      "  time_this_iter_s: 44.38364291191101\n",
      "  time_total_s: 2565.447763442993\n",
      "  timestamp: 1579202124\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 54\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2565 s, 54 iter, 108000 ts, 55.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-16-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 100.52111805444753\n",
      "  episode_reward_mean: 56.64508224857329\n",
      "  episode_reward_min: -17.050837169789244\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1100\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5141.363\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5034844875335693\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028893237933516502\n",
      "        policy_loss: -0.032058343291282654\n",
      "        total_loss: 239.9851837158203\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 240.0079803466797\n",
      "    load_time_ms: 1.903\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "    sample_time_ms: 41395.159\n",
      "    update_time_ms: 11.129\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.74084507042254\n",
      "    ram_util_percent: 25.87183098591549\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.96301463098744\n",
      "    mean_inference_ms: 8.261579581911317\n",
      "    mean_processing_ms: 13.587681176186742\n",
      "  time_since_restore: 2614.8909385204315\n",
      "  time_this_iter_s: 49.443175077438354\n",
      "  time_total_s: 2614.8909385204315\n",
      "  timestamp: 1579202174\n",
      "  timesteps_since_restore: 110000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 55\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2614 s, 55 iter, 110000 ts, 56.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-16-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 100.52111805444753\n",
      "  episode_reward_mean: 57.72877420066659\n",
      "  episode_reward_min: -17.050837169789244\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1120\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5105.708\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2542585134506226\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021174151450395584\n",
      "        policy_loss: -0.03285069763660431\n",
      "        total_loss: 213.80796813964844\n",
      "        vf_explained_var: 2.980232238769531e-07\n",
      "        vf_loss: 213.83399963378906\n",
      "    load_time_ms: 1.875\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    sample_time_ms: 40951.414\n",
      "    update_time_ms: 10.878\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.32542372881356\n",
      "    ram_util_percent: 25.844067796610176\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.90247971262856\n",
      "    mean_inference_ms: 8.26082529083967\n",
      "    mean_processing_ms: 13.5829013976147\n",
      "  time_since_restore: 2656.6553304195404\n",
      "  time_this_iter_s: 41.76439189910889\n",
      "  time_total_s: 2656.6553304195404\n",
      "  timestamp: 1579202215\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 56\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2656 s, 56 iter, 112000 ts, 57.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-17-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 100.52111805444753\n",
      "  episode_reward_mean: 60.08025274934013\n",
      "  episode_reward_min: -17.050837169789244\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1140\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5094.885\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3620946407318115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021114744246006012\n",
      "        policy_loss: -0.032542694360017776\n",
      "        total_loss: 259.99749755859375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 260.0232849121094\n",
      "    load_time_ms: 1.888\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "    sample_time_ms: 40771.816\n",
      "    update_time_ms: 10.366\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.946875\n",
      "    ram_util_percent: 25.823437500000004\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.841538747943915\n",
      "    mean_inference_ms: 8.260477496544015\n",
      "    mean_processing_ms: 13.577602972134391\n",
      "  time_since_restore: 2701.7464830875397\n",
      "  time_this_iter_s: 45.09115266799927\n",
      "  time_total_s: 2701.7464830875397\n",
      "  timestamp: 1579202261\n",
      "  timesteps_since_restore: 114000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 57\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2701 s, 57 iter, 114000 ts, 60.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-18-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 100.52111805444753\n",
      "  episode_reward_mean: 60.784321530858776\n",
      "  episode_reward_min: -17.050837169789244\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1160\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5042.037\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4415888786315918\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017892563715577126\n",
      "        policy_loss: -0.030491912737488747\n",
      "        total_loss: 233.12142944335938\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 233.14608764648438\n",
      "    load_time_ms: 1.755\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    sample_time_ms: 40175.122\n",
      "    update_time_ms: 10.565\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.308333333333334\n",
      "    ram_util_percent: 25.848333333333343\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.7895658778526\n",
      "    mean_inference_ms: 8.26007463083153\n",
      "    mean_processing_ms: 13.572662177233783\n",
      "  time_since_restore: 2743.421759366989\n",
      "  time_this_iter_s: 41.67527627944946\n",
      "  time_total_s: 2743.421759366989\n",
      "  timestamp: 1579202302\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 58\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2743 s, 58 iter, 116000 ts, 60.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-19-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 102.85554718204163\n",
      "  episode_reward_mean: 62.46273632253943\n",
      "  episode_reward_min: -5.0139518459636685\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1180\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5009.61\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0051480531692505\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.030435260385274887\n",
      "        policy_loss: -0.041746627539396286\n",
      "        total_loss: 268.28887939453125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 268.32098388671875\n",
      "    load_time_ms: 1.761\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "    sample_time_ms: 40461.594\n",
      "    update_time_ms: 10.802\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.39\n",
      "    ram_util_percent: 26.29857142857143\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.743327075610296\n",
      "    mean_inference_ms: 8.257856864725078\n",
      "    mean_processing_ms: 13.567568699980582\n",
      "  time_since_restore: 2792.621803998947\n",
      "  time_this_iter_s: 49.20004463195801\n",
      "  time_total_s: 2792.621803998947\n",
      "  timestamp: 1579202352\n",
      "  timesteps_since_restore: 118000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 59\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.4/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2792 s, 59 iter, 118000 ts, 62.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-19-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 102.85554718204163\n",
      "  episode_reward_mean: 62.95971180877482\n",
      "  episode_reward_min: 2.8689018161318103\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1200\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5085.098\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5393633842468262\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.029807785525918007\n",
      "        policy_loss: -0.03823345899581909\n",
      "        total_loss: 212.81015014648438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 212.8388214111328\n",
      "    load_time_ms: 1.771\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    sample_time_ms: 40666.375\n",
      "    update_time_ms: 10.63\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.73676470588235\n",
      "    ram_util_percent: 26.445588235294114\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.69289040644572\n",
      "    mean_inference_ms: 8.25269524891469\n",
      "    mean_processing_ms: 13.562293477966202\n",
      "  time_since_restore: 2840.3072984218597\n",
      "  time_this_iter_s: 47.6854944229126\n",
      "  time_total_s: 2840.3072984218597\n",
      "  timestamp: 1579202399\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 60\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.4/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2840 s, 60 iter, 120000 ts, 63 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-20-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 102.85554718204163\n",
      "  episode_reward_mean: 61.22903682473514\n",
      "  episode_reward_min: 5.189312512280547\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1220\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5054.73\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5184645652770996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0290670208632946\n",
      "        policy_loss: -0.040447838604450226\n",
      "        total_loss: 167.9401397705078\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 167.97128295898438\n",
      "    load_time_ms: 1.768\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "    sample_time_ms: 40375.136\n",
      "    update_time_ms: 10.502\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.853125000000006\n",
      "    ram_util_percent: 26.395312500000003\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.657471537202056\n",
      "    mean_inference_ms: 8.248053442529491\n",
      "    mean_processing_ms: 13.558266917074775\n",
      "  time_since_restore: 2884.9871230125427\n",
      "  time_this_iter_s: 44.67982459068298\n",
      "  time_total_s: 2884.9871230125427\n",
      "  timestamp: 1579202444\n",
      "  timesteps_since_restore: 122000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 61\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.4/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2884 s, 61 iter, 122000 ts, 61.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-21-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 102.85554718204163\n",
      "  episode_reward_mean: 59.414503470996685\n",
      "  episode_reward_min: 5.189312512280547\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1240\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5071.51\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3970274925231934\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018145285546779633\n",
      "        policy_loss: -0.030347038060426712\n",
      "        total_loss: 224.2925262451172\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 224.31707763671875\n",
      "    load_time_ms: 1.812\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    sample_time_ms: 40575.127\n",
      "    update_time_ms: 10.576\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.53768115942029\n",
      "    ram_util_percent: 26.579710144927535\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.61769730671419\n",
      "    mean_inference_ms: 8.234805857311416\n",
      "    mean_processing_ms: 13.55646762245085\n",
      "  time_since_restore: 2933.4070389270782\n",
      "  time_this_iter_s: 48.41991591453552\n",
      "  time_total_s: 2933.4070389270782\n",
      "  timestamp: 1579202492\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 62\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.5/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2933 s, 62 iter, 124000 ts, 59.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-22-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 102.85554718204163\n",
      "  episode_reward_mean: 58.52697506205848\n",
      "  episode_reward_min: 5.189312512280547\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1260\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5078.02\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1390745639801025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020847976207733154\n",
      "        policy_loss: -0.03327665477991104\n",
      "        total_loss: 221.42156982421875\n",
      "        vf_explained_var: 1.7881393432617188e-07\n",
      "        vf_loss: 221.44818115234375\n",
      "    load_time_ms: 1.81\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "    sample_time_ms: 40659.256\n",
      "    update_time_ms: 10.691\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.752307692307696\n",
      "    ram_util_percent: 26.915384615384614\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.622161648547554\n",
      "    mean_inference_ms: 8.230346909708391\n",
      "    mean_processing_ms: 13.557165375728337\n",
      "  time_since_restore: 2978.6755566596985\n",
      "  time_this_iter_s: 45.26851773262024\n",
      "  time_total_s: 2978.6755566596985\n",
      "  timestamp: 1579202538\n",
      "  timesteps_since_restore: 126000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 63\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.5/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 2978 s, 63 iter, 126000 ts, 58.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-23-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 95.23665560877598\n",
      "  episode_reward_mean: 55.911578370582994\n",
      "  episode_reward_min: -6.449340134295835\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1280\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5127.368\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.373891830444336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016615040600299835\n",
      "        policy_loss: -0.026396118104457855\n",
      "        total_loss: 222.0739288330078\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 222.0950164794922\n",
      "    load_time_ms: 1.761\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    sample_time_ms: 41109.665\n",
      "    update_time_ms: 10.723\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.68142857142856\n",
      "    ram_util_percent: 26.159999999999997\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.62552354527042\n",
      "    mean_inference_ms: 8.222708381219672\n",
      "    mean_processing_ms: 13.558492089742481\n",
      "  time_since_restore: 3028.05584859848\n",
      "  time_this_iter_s: 49.38029193878174\n",
      "  time_total_s: 3028.05584859848\n",
      "  timestamp: 1579202587\n",
      "  timesteps_since_restore: 128000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 64\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 3028 s, 64 iter, 128000 ts, 55.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-23-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.35796516781592\n",
      "  episode_reward_mean: 54.38289538872125\n",
      "  episode_reward_min: -6.449340134295835\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1300\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5140.353\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.374695897102356\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02572917938232422\n",
      "        policy_loss: -0.0341593399643898\n",
      "        total_loss: 197.58592224121094\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 197.61184692382812\n",
      "    load_time_ms: 1.76\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "    sample_time_ms: 40851.55\n",
      "    update_time_ms: 10.645\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.891044776119408\n",
      "    ram_util_percent: 25.992537313432837\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.6001237849214\n",
      "    mean_inference_ms: 8.212828586451327\n",
      "    mean_processing_ms: 13.557748088354261\n",
      "  time_since_restore: 3075.0467932224274\n",
      "  time_this_iter_s: 46.990944623947144\n",
      "  time_total_s: 3075.0467932224274\n",
      "  timestamp: 1579202634\n",
      "  timesteps_since_restore: 130000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 65\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 3075 s, 65 iter, 130000 ts, 54.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-24-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.35796516781592\n",
      "  episode_reward_mean: 54.946470602473674\n",
      "  episode_reward_min: -6.449340134295835\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1320\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5169.028\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3918033838272095\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02013370953500271\n",
      "        policy_loss: -0.033619172871112823\n",
      "        total_loss: 198.51527404785156\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 198.5423126220703\n",
      "    load_time_ms: 1.8\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    sample_time_ms: 41398.653\n",
      "    update_time_ms: 10.606\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.72941176470589\n",
      "    ram_util_percent: 25.983823529411772\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.583234508051916\n",
      "    mean_inference_ms: 8.205192257333799\n",
      "    mean_processing_ms: 13.55664004402812\n",
      "  time_since_restore: 3122.5687131881714\n",
      "  time_this_iter_s: 47.52191996574402\n",
      "  time_total_s: 3122.5687131881714\n",
      "  timestamp: 1579202682\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 66\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 3122 s, 66 iter, 132000 ts, 54.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-25-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.35796516781592\n",
      "  episode_reward_mean: 55.282588524138475\n",
      "  episode_reward_min: -6.449340134295835\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1340\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5195.984\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.341963529586792\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0253859031945467\n",
      "        policy_loss: -0.034297823905944824\n",
      "        total_loss: 225.4811248779297\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 225.50735473632812\n",
      "    load_time_ms: 1.807\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "    sample_time_ms: 41603.012\n",
      "    update_time_ms: 10.753\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.24776119402985\n",
      "    ram_util_percent: 25.956716417910446\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.58440206823342\n",
      "    mean_inference_ms: 8.205321890341299\n",
      "    mean_processing_ms: 13.555096375015165\n",
      "  time_since_restore: 3169.9755868911743\n",
      "  time_this_iter_s: 47.40687370300293\n",
      "  time_total_s: 3169.9755868911743\n",
      "  timestamp: 1579202729\n",
      "  timesteps_since_restore: 134000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 67\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 3169 s, 67 iter, 134000 ts, 55.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-26-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 98.93548797220063\n",
      "  episode_reward_mean: 55.83685809596598\n",
      "  episode_reward_min: -6.449340134295835\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1360\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5215.691\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1667276620864868\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.029096432030200958\n",
      "        policy_loss: -0.03866886347532272\n",
      "        total_loss: 238.896484375\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 238.92584228515625\n",
      "    load_time_ms: 1.809\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    sample_time_ms: 41845.992\n",
      "    update_time_ms: 10.713\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.1390625\n",
      "    ram_util_percent: 26.054687499999996\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.583203590008466\n",
      "    mean_inference_ms: 8.206157660496016\n",
      "    mean_processing_ms: 13.55270958608967\n",
      "  time_since_restore: 3214.277384519577\n",
      "  time_this_iter_s: 44.30179762840271\n",
      "  time_total_s: 3214.277384519577\n",
      "  timestamp: 1579202773\n",
      "  timesteps_since_restore: 136000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 68\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 3214 s, 68 iter, 136000 ts, 55.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-27-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 98.93548797220063\n",
      "  episode_reward_mean: 55.96532391534245\n",
      "  episode_reward_min: -1.9539690867471913\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1380\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5225.197\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3977524042129517\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04960419982671738\n",
      "        policy_loss: -0.05066853389143944\n",
      "        total_loss: 219.58203125\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 219.6167449951172\n",
      "    load_time_ms: 1.97\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "    sample_time_ms: 41906.36\n",
      "    update_time_ms: 10.621\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.238028169014086\n",
      "    ram_util_percent: 26.014084507042252\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.60622685661837\n",
      "    mean_inference_ms: 8.213974173561015\n",
      "    mean_processing_ms: 13.551429315175156\n",
      "  time_since_restore: 3264.176303625107\n",
      "  time_this_iter_s: 49.898919105529785\n",
      "  time_total_s: 3264.176303625107\n",
      "  timestamp: 1579202823\n",
      "  timesteps_since_restore: 138000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 69\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 3264 s, 69 iter, 138000 ts, 56 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv2-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-01-16_21-27-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 98.93548797220063\n",
      "  episode_reward_mean: 58.958987750703116\n",
      "  episode_reward_min: -1.9539690867471913\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1400\n",
      "  experiment_id: c7d50086310c4570b32a5fddedbcaced\n",
      "  hostname: victor-M5400\n",
      "  info:\n",
      "    grad_time_ms: 5197.114\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.48054200410842896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8822353482246399\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01628127694129944\n",
      "        policy_loss: -0.029177101328969002\n",
      "        total_loss: 261.1326599121094\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 261.1540832519531\n",
      "    load_time_ms: 1.956\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    sample_time_ms: 42027.53\n",
      "    update_time_ms: 10.512\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.0.100\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.391304347826086\n",
      "    ram_util_percent: 26.079710144927525\n",
      "  pid: 5315\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 36.65896509803606\n",
      "    mean_inference_ms: 8.225178120394943\n",
      "    mean_processing_ms: 13.552456054701826\n",
      "  time_since_restore: 3312.7931656837463\n",
      "  time_this_iter_s: 48.616862058639526\n",
      "  time_total_s: 3312.7931656837463\n",
      "  timestamp: 1579202872\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 70\n",
      "  trial_id: 8791ae4e\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.7 GB\n",
      "Result logdir: /home/victor/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv2-v0_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=5315], 3312 s, 70 iter, 140000 ts, 59 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-16 21:27:59,996\tERROR worker.py:1716 -- listen_error_messages_raylet: Error 111 connecting to 192.168.0.100:48810. Connection refused.\n",
      "2020-01-16 21:27:59,998\tERROR worker.py:1616 -- print_logs: Error 111 connecting to 192.168.0.100:48810. Connection refused.\n",
      "2020-01-16 21:28:00,001\tERROR import_thread.py:89 -- ImportThread: Error 111 connecting to 192.168.0.100:48810. Connection refused.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-235010303bf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"max_failures\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \"stop\": {  # stopping conditions\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;34m\"training_iteration\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# number of iterations to stop after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         },\n\u001b[1;32m     14\u001b[0m     },\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(experiments, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mtrial_executor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             return_trials=True)\n\u001b[0m\u001b[1;32m    325\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init, sync_function)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mlast_debug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_debug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mDEBUG_PRINT_INTERVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_running_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_available_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process_trial\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mresult_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0mwait_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mNONTRIVIAL_WAIT_TIME_THRESHOLD_S\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_ids, num_returns, timeout)\u001b[0m\n\u001b[1;32m   2370\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2371\u001b[0m             \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2372\u001b[0;31m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2373\u001b[0m         )\n\u001b[1;32m   2374\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.RayletClient.wait\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/site-packages/ray/exceptions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, client_exc)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_exc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,\n",
    "        \"env\": gym_name,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"checkpoint_at_end\": True,  # generate a checkpoint at the end\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 500,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
